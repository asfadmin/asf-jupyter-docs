{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to OpenSARlab What is OpenSARlab? OpenSARlab is a service providing users persistent, cloud-based, customizable computing environments. It insures that groups of scientists and students have access to identical environments, containing the same software, running on the same hardware. It operates in the cloud, which means anyone with a moderately reliable internet connection can access their development environment. OpenSARlab sits alongside ASF's data archives in AWS, allowing for low latency transfer of large data products. OpenSARlab is a deployable service that creates an autoscaling Kubernetes cluster in Amazon AWS, running JupyterHub. Users have access to customizable environments running JupyterLab via authenticated accounts with persistent storage. While OpenSARlab was designed with SAR data science in mind, it is not limited to this field. Any group development scenario involving large datasets and/or the need for complicated development environments can benefit from working in an OpenSARlab deployment. How will OpenSARlab benefit my work as a SAR scientist? OpenSARlab addresses the following issues that often arise when developing SAR data science techniques, especially in a collaborative setting: Most SAR analysis algorithms require the installation of many interdependent Python science packages Collaboration is often slowed or interrupted when contributors work in varying environments with different versions of installed dependencies SAR data products are often quite large, which leads to slow, expensive data transfers SAR scientists with limited resources may lack access to the hardware required for analysis How will OpenSARlab benefit the class or training I am planning? OpenSARlab alleviates some of the pitfalls commonly encountered when teaching software development and data science in any field: Teaching is often interrupted when students work in varying environments, requiring valuable instructor time to help set up their systems so they may complete their assignments. Students may lack the hardware needed to run the software required for assignments. Students may lack the bandwidth needed to download large data products to their local computers. How is OpenSARlab different from Binder? Authenticated user accounts User group management Persistent user storage Cost reducing storage management features Customizable server resources (pick your EC2 size) Deployable to other AWS accounts Developer defined server timeouts (not restricted to 10 minutes of inactivity) How to Access OpenSARlab As a Paid Service Managed by Alaska Satellite Facility Enterprise Contact ASF-E to discuss options for setting up an OpenSARlab deployment to suit your needs. Deploy OpenSARlab to Your Own AWS Account (Coming Soon) Take our publicly accessible codebase and create your own, self-managed deployments in Amazon AWS. Contact Us Have questions, suggestions, or need advice? We would love to hear from you! Email us at uaf-jupyterhub-asf@alaska.edu .","title":"Home"},{"location":"#welcome-to-opensarlab","text":"","title":"Welcome to OpenSARlab"},{"location":"#what-is-opensarlab","text":"OpenSARlab is a service providing users persistent, cloud-based, customizable computing environments. It insures that groups of scientists and students have access to identical environments, containing the same software, running on the same hardware. It operates in the cloud, which means anyone with a moderately reliable internet connection can access their development environment. OpenSARlab sits alongside ASF's data archives in AWS, allowing for low latency transfer of large data products. OpenSARlab is a deployable service that creates an autoscaling Kubernetes cluster in Amazon AWS, running JupyterHub. Users have access to customizable environments running JupyterLab via authenticated accounts with persistent storage. While OpenSARlab was designed with SAR data science in mind, it is not limited to this field. Any group development scenario involving large datasets and/or the need for complicated development environments can benefit from working in an OpenSARlab deployment.","title":"What is OpenSARlab?"},{"location":"#how-will-opensarlab-benefit-my-work-as-a-sar-scientist","text":"OpenSARlab addresses the following issues that often arise when developing SAR data science techniques, especially in a collaborative setting: Most SAR analysis algorithms require the installation of many interdependent Python science packages Collaboration is often slowed or interrupted when contributors work in varying environments with different versions of installed dependencies SAR data products are often quite large, which leads to slow, expensive data transfers SAR scientists with limited resources may lack access to the hardware required for analysis","title":"How will OpenSARlab benefit my work as a SAR scientist?"},{"location":"#how-will-opensarlab-benefit-the-class-or-training-i-am-planning","text":"OpenSARlab alleviates some of the pitfalls commonly encountered when teaching software development and data science in any field: Teaching is often interrupted when students work in varying environments, requiring valuable instructor time to help set up their systems so they may complete their assignments. Students may lack the hardware needed to run the software required for assignments. Students may lack the bandwidth needed to download large data products to their local computers.","title":"How will OpenSARlab benefit the class or training I am planning?"},{"location":"#how-is-opensarlab-different-from-binder","text":"Authenticated user accounts User group management Persistent user storage Cost reducing storage management features Customizable server resources (pick your EC2 size) Deployable to other AWS accounts Developer defined server timeouts (not restricted to 10 minutes of inactivity)","title":"How is OpenSARlab different from Binder?"},{"location":"#how-to-access-opensarlab","text":"","title":"How to Access OpenSARlab"},{"location":"#as-a-paid-service-managed-by-alaska-satellite-facility-enterprise","text":"Contact ASF-E to discuss options for setting up an OpenSARlab deployment to suit your needs.","title":"As a Paid Service Managed by Alaska Satellite Facility Enterprise"},{"location":"#deploy-opensarlab-to-your-own-aws-account-coming-soon","text":"Take our publicly accessible codebase and create your own, self-managed deployments in Amazon AWS.","title":"Deploy OpenSARlab to Your Own AWS Account (Coming Soon)"},{"location":"#contact-us","text":"Have questions, suggestions, or need advice? We would love to hear from you! Email us at uaf-jupyterhub-asf@alaska.edu .","title":"Contact Us"},{"location":"dev/","text":"System Diagram Deploy OpenSARlab to AWS","title":"Dev"},{"location":"release_notes/","text":"June 2021","title":"Release notes"},{"location":"user/","text":"Welcome to the OpenSARlab User Guide Jupyter Notebook Intro Running Jupyter Notebooks Jupyter Magic Commands OpenSARlab Account Details Git in OpenSARlab OpenSARlab Terminal OpenSARlab Servers and Kernels Jupyter Notebook Extensions Installing Software in OpenSARlab Conda Environments Logging Out and Server Shutdown Troubleshooting Guide","title":"Welcome to the OpenSARlab User Guide"},{"location":"user/#welcome-to-the-opensarlab-user-guide","text":"Jupyter Notebook Intro Running Jupyter Notebooks Jupyter Magic Commands OpenSARlab Account Details Git in OpenSARlab OpenSARlab Terminal OpenSARlab Servers and Kernels Jupyter Notebook Extensions Installing Software in OpenSARlab Conda Environments Logging Out and Server Shutdown Troubleshooting Guide","title":"Welcome to the OpenSARlab User Guide"},{"location":"dev-guides/deploy_OpenSARlab/","text":"Return to Developer Guide Deploy OpenSARlab to an AWS account A note about deployments: A deployment of OpenSARlab refers to a standalone instance of OpenSARlab. If you are setting up OpenSARlab for several classes and/or collaborative groups with disparate needs or funding sources, it may be useful to give them each their own standalone deployment. This separates user group authentication, simplifies billing for each group, and allows for easy cleanup at the end of a project or class (just delete the deployment). In the following instructions, replace any occurrence of \" \" with the deployment name you have chosen. Make your deployment name lowercase and use no special characters other than dashes (-). It will be used to generate part of the Cognito callback URL and CloudFormation stack names also follow the same naming convention. Take AWS SES out of sandbox The AWS Simple Email Service is used by OpenSARlab to send emails to users and administrators. These include authentication related notifications and storage lifecycle management messages. While SES is in sandbox, you are limited to sending 1 email per second with no more than 200 in a 24 hour period, and they may only be sent from an SES verified address to other SES verified addresses. Note: Provide a detailed explanation of your SES use and email policies when applying to exit the sandbox or you will be denied. Approval can take 24-48 hours Follow these instructions to take your SES out of sandbox. Create an AWS Cost Allocation Tag Note: only management accounts can create cost allocation tags Create a cost allocation tag or have one created by someone with access Give it an available name that makes sense for tracking deployment names associated with AWS resources i.e. \"deployment_name\" Add dockerhub credentials to AWS Secrets Manager This deployment uses a few publicly available docker images. Due to dockerhub rate limits ( https://www.docker.com/increase-rate-limits ), you will need to set up a dockerhub account. A free-tier account will suffice. CodePipeline's ip address is shared by many users and you will likely hit the rate limit as an anonymous user ( details here ). Note: By default this secret will be used for multiple deployments. Optionally, you could edit the codebuild section in the cf-cluster.yml to point to a different secret. If you don't have a dockerhub account, create one here Open the AWS Secrets Manager console Click the \"Store a new secret\" button Page 1: Select \"Other type of secrets\" Select the \"Plaintext\" tab Delete the default content Add your username and password, separated by a space Example: username password Click the \"Next\" button Page 2: Secret name dockerhub/creds Click the \"Next\" button Page 3: Click the \"Next\" button Page 4: Click the \"Store\" button Setup an iCal calendar for notifications Notifications are generated from iCal calendar events. ASF uses Google Calendar but any publicly accessible iCal formatted calendar should work as well Create a public iCal formatted calendar The iCal formatted url will be needed in later Notification calendar events must be properly formatted. Formatting details available in the Take care of odds and ends section Store your CA certificate OpenSARlab will lack full functionality if not using https (SSL certification) Follow these instructions to import your CA certificate into the AWS Certificate Manager Prepare CodeCommit Repos TODO Do this differently All the public OpenSARlab repos are in the ASFOpenSARlab Github Org Create a -container CodeCommit repo in your AWS account Create a -cluster CodeCommit repo Clone the -container and -cluster repos to your local computer using ssh cd into your local -container repo add ASFOpenSARlab/opensarlab-container as a remote on your local -container repo git remote add github https://github.com/ASFOpenSARlab/opensarlab-container.git Pull the remote opensarlab-container repo into your local -container repo git pull github main Create a main branch in the -container repo git checkout -b main Push to the remote -container repo git push origin main cd into your local -cluster repo add ASFOpenSARlab/opensarlab-cluster as a remote on your local -cluster repo git remote add github https://github.com/ASFOpenSARlab/opensarlab-cluster.git Pull the remote opensarlab-cluster repo into your local -cluster repo git pull github main Create a main branch in the -cluster repo git checkout -b main Push to the remote -cluster repo git push origin main You should now have container and cluster repos in CodeCommit that are duplicates of those found in ASFOpenSARlab Create an S3 bucket to hold the lambda handler script Create an S3 bucket in your AWS account called -lambda Alternatively, you could use the same bucket to hold lambdas for multiple deployments Customize opensarlab_container code for deployment The opensarlab-container repo contains one example image named \"sar\", which you can reference when creating new images. Images can be used by multiple profiles Note: It is easiest to work in your local repo and push your changes when you're done. Duplicate the images/sar directory and rename it, using your chosen image name Create and add any additional custom jupyter magic commands to the jupyter-hooks/custom_magics directory Add any additional scripts you may have created for use in your image to the scripts directory Add any image test scripts to tests directory Edit images/ /jupyter-hooks/sar.sh Rename sar.sh to .sh Copy any additional custom Jupyter magic scripts to $HOME/.ipython/image_default/startup/ (alongside 00-df.py) Edit the repos being pulled to suit your deployment and image needs Edit images/ /build.sh Change 6 $DOCKER_REGISTRY namespaces from \"sar\" to your image namespace Repeat the previous step, adding scripts for any additional images Edit cf-container.yaml SARRepository Change the resource name (SARRepository) so it makes sense given the image that will use it Change the \"/sar\" portion of RepositoryName to use a namespace matching your image name Pipeline Name: !Sub ${AWS::StackName}-Build-Images Change \"- Name: images-sar\" to \"- Name: images- \" EnvironmentVariables Change the IMAGE_NAME value from \"sar\" to \" Add stages for any additional images Edit dockerfile Adjust the packages in the 2nd apt install command to suit your image needs Add any pip packages you wish installed in the base conda environment Add any conda packages you wish installed in the base conda environment Create any conda environments you would like pre-installed before \"USER jovyan\" If using environment.yml files, store them in an \"envs\" directory in /jupyter-hooks, and they will be copied into the container RUN conda env create -f /etc/jupyter-hooks/envs/ _env.yml --prefix /etc/jupyter-hooks/envs/ Run any tests for this image that you added to the tests directory Remove the images/sar directory and sar.sh test script, unless you plan to use the sar image Add a test script for your image use sar.sh as an example name it .sh Edit tests/finale.sh Delete \"bash sar.sh\" if not using it Add \"bash .sh\" Add, commit, and push changes to the remote CodeCommit repo Customize opensarlab_cluster code for deployment The opensarlab-cluster repo contains TODO comments everywhere deployment specific edits should be made Note: Most IDEs have functionality to easily locate and organize TODOs. Searching the code for \"TODO\" will also work. hub/dockerfile Add your organization's name as MAINTAINER Adjust the images (jpgs, pngs, etc...) and templates being copied to the docker image hub/etc/jupyterhub/custom/delete_snapshot.py Edit admin email addresses (2 locations) hub/usr/local/share/jupyterhub/templates/login.html Edit the images and messages that appear on the login page hub/usr/local/share/jupyterhub/templates/pending.html Edit the message to users that their account is pending approval notifications/dock Add the ICAL_URL of your notification calendar notifications/dockerfile Add your organization's name as MAINTAINER cf-cluster.yaml Add a NodeInstanceType parameter for every EC2 type must be alphanumeric Remove the example NodeInstanceTypePROFILE1 resource Add a LaunchConfiguration for every NodeInstanceType must be alphanumeric InstanceType should match the NodeInstanceType created above The server_type in UserData must match profile's server_type that you will use in helm_config.yaml Add an AutoScalingGroup resource for every NodeInstanceType must be alphanumeric LaunchConfigurationName should match new LaunchConfiguration created above Remove the example AutoScalingGroupPROFILE1 resource Remove the example LaunchConfigurationPROFILE1 resource helm_config.yaml Add new profiles, using the example PROFILE_1 as a template Reference the kubespawner docs for more options and details Change the name of the profile being search for in group_list Change the display_name Change the profile description Change the extra_labels and node_selector server_types to match the server_type used in the profiles LaunchConfiguration in cf-cluster.yaml Adjust the path to the postStart lifecycle hook Adjust the kubespawner_override args --NotebookApp.jinja_template_vars PROFILE_NAME to the correct profile name Adjust the mem_limit The maximum amount of memory available to each user <= memory available for EC2 type Adjust the mem_guarantee (or cpu_guarantee) The minimum amount of memory guaranteed to each user If there is not enough memory on any existing node, the autoscaler will spin up a new node Use the mem_guarantee to determine how nodes should be shared among users Even if not sharing nodes, do not guarantee all available memory The node requires some memory for setup (varies and may take some trial and error to figure out how much to reserve) Adjust the cpu_guarantee (or mem_guarantee) The minimum EC2 cpu units guaranteed to each user If there aren't enough cpu units left on a node for the next user, the autoscaler will spin up a new node Use the cpu_guarantee to determine how nodes should be shared among users Even if not sharing nodes, do not guarantee all available cpus The node requires some memory for setup Adjust the storage capacity This should match the storage capacity used for all profiles You can increase volume sizes at a later date Reducing volume sizes is not advised due to a high likelihood of data loss Remove the example PROFILE_1 lambda_handler.py Lambdas are used by Cognito event triggers for logging and emailing notifications to users and administrators Create a lambda_handler.py file based on lambda_handler.py.example Adjust email messages to suit the needs of the deployment zip the file, creating lambda_handler.py.zip be sure to keep the \".py\" as part of the filename Upload the zip to the -lambda S3 bucket After setting up Cognito for the first time, anytime you make changes to this file you will need to: Change the name of the zip file Do not change the name of the lambda_handler.py file it contains Upload it to the lambda S3 bucket Update the EmailLambdaKeyName parameter in the cognito CloudFormation template to match the new filename After updating the pipeline, set all Cognito triggers to 'None', save them, set them back to the correct lambdas, and save them again Add, commit, and push changes to the remote CodeCommit repo Build the Cognito CloudFormation stack Open CloudFormation in the AWS console Click the \"Create stack\" button and select \"With new resources (standard)\" Page 1 : Create stack Under \"Specify template\", check \"Upload a template file\" Use the file chooser to select cf-cognito.py from your local branch of the -cluster repo Click the \"Next\" button Page 2: Specify stack details Stack name: -auth dashes (-) are the only allowed special characters lowercase only AdminEmailAddress: SES verified email address of the primary administrator This is the sender address users will see on confirmation, verification, and volume lifecycle emails AdminEmailSNSArn: Arn of the above admin email address Must be AWS SES verified (easy to do in the Amazon Simple Email Service console) DeploymentURL Enter the deployment domain, if known (i.e. https://deployment_name.your_domain.tdl) The placeholder domain can be left in place temporarily if the actual domain is not yet known CostTagValue EmailLambdaBucketName -lambda EmailLambdaKeyName lambda_handler.py.zip (or whatever you named it) Click the \"Next\" button Page 3: Configure stack options Tags: Key: Cost allocation tag Value: Click the \"Next\" button Page 4: Review -auth Review and confirm correctness Check the box next to \"I acknowledge that AWS CloudFormation might create IAM resources\" Click the \"Create Stack Button\" Monitor the stack build for errors and rollbacks The screen does not self-update Use the refresh buttons If the build fails and rolls back goto the CloudFormation stacks page select and delete the failed stack before correcting any errors and trying again Build the container CloudFormation stack This will create the hub image, images for each profile, and store them in namespaced ECR repos Open CloudFormation in the AWS console Click the \"Create stack\" button and select \"With new resources (standard)\" Page 1 : Create stack Under \"Specify template\", check \"Upload a template file\" Use the file chooser to select cf-container.py from your local branch of the -container repo Click the \"Next\" button Page 2: Specify stack details Stack name: -container CodeCommitSourceBranch: The name of the production branch of the -container CodeCommit repo CodeCommitSourceRepo: -container CostTagValue Page 3: Configure stack options Tags: Key: Cost allocation tag Value: Click the \"Next\" button Page 4: Review -auth Review and confirm correctness Check the box next to \"I acknowledge that AWS CloudFormation might create IAM resources\" Click the \"Create Stack Button\" Monitor the stack build for errors and rollbacks The screen does not self-update Use the refresh buttons If the build fails and rolls back goto the CloudFormation stacks page select and delete the failed stack before correcting any errors and trying again When stack creation is complete, open CodePipeline in the AWS console Open the -container-Container-Pipeline pipeline and monitor it as it runs Click the \"details\" link under each stage action for a closer inspection If part of the pipeline fails Identify the issue from the \"details\" output Correct the code Update the -container repo Go to the pipeline Click the \"Release change\" button Build the cluster CloudFormation stack This CloudFormation stack dynamically creates a second CloudFormation stack. You will end up with a stack and a -cluster stack. Open CloudFormation in the AWS console Page 1 : Create stack Click the \"Create stack\" button and select \"With new resources (standard)\" Under \"Specify template\", check \"Upload a template file\" Use the file chooser to select cf-pipeline.py from your local branch of the repo Click the \"Next\" button Page 2: Specify stack details Stack name: -cluster AdminUserName: JupyterHub Admin username Initial default admin with access to the JupyterHub admin and group pages CertificateArn: Arn associated with the CA certificate you stored in AWS Certificate Manager arn:aws:acm: : :certificate/ CodeCommitRepoName: Name of the CodeCommit repo holding your -cluster code CodeCommitBranchName: Name of the branch holding this deployment's cluster code ContainerNamespace: -container CostTagKey: Cost allocation tag CostTagValue: ICALUrl: The iCal formatted URL of the calendar used for notifications DeploymentURL: Your custom URL (should match DeploymentURL parameter in -auth stack) If left blank, the default load balancer will be used Can be updated later OAuthPoolName: -auth Page 3: Configure stack options Tags: Key: Cost allocation tag Value: Click the \"Next\" button Page 4: Review -auth Review and confirm correctness Check the box next to \"I acknowledge that AWS CloudFormation might create IAM resources\" Click the \"Create Stack\" button Open CodePipeline in the AWS console Open the -Pipeline pipeline and monitor it as it runs Click the \"details\" link under each stage action for a closer inspection Take care of odds and ends Update the DeploymentURL parameters in 2 CloudFormation stacks with load balancer URL or your custom URL If you already entered a custom URL when building the stacks, skip this step If using the load balancer URL Find it in the 'Outputs' tab of the CloudFormation console for the -cluster CloudFormation stack Note: you will have to prepend \"https://\" to it Update the stack Navigate to the stack CloudFormation console Click the 'Update' button Page 1: Leave 'Use current template' selected Click the 'Next' button Page 2: Update the DeploymentURL parameter Click the 'Next' button Page 3: Click the 'Next' button Page 4: Review and confirm correctness Check the box next to \"I acknowledge that AWS CloudFormation might create IAM resources\" Click the \"Update Stack\" button Repeat the above process for the -auth stack Add the cost allocation tag to the EKS cluster Navigate to the AWS EKS console click the \"Clusters\" link in the sidebar menu Click on -cluster Click the \"Tags\" tab Click the \"Manage tags\" button Click the \"Add tag\" button Key: Cost allocation tag Value: Wrap things up in Cognito Navigate to the AWS Cognito console Click the \"Manage User Pools\" button Click the \" -auth\" user pool button Add the cost allocation tag to the Cognito user pool Click the \"Tags\" sidebar menu link Click the \"Add tag\" link Tag Key: Cost allocation tag Tag Value: Click the \"Save changes\" button Reset triggers Cognito triggers can fall out of sync with email and logging lambdas so it is safest to reset them Click the \"Triggers\" sidebar menu link Set all lambda functions to \"none\" Click the \"Save changes\" button Click the \"Triggers\" sidebar menu link Pre sign-up: select the -auth-LogFunction- lambda Pre authentication: select the -auth-PreAuthFunction- lambda Post authentication: select the -auth-LogFunction- lambda Post confirmation: select the -auth-PostEmailVerificationFun- lambda Pre Token Generator: select the -auth-LogFunction- lambda Leave the remaining triggers set to \"none\" Click the \"Save changes\" button Update image tags for every profile in helm_config.yml ( -cluster repository) hub:image:tag 'latest' will only work for the hub:image:tag on the first build It is safest to never use the \"latest\" tag Use the image's ECR timestamp Found in the AWS Elastic Container Service console profile image_url tag update the tag at the end of the ECR image_url Use the image's 7 character long ECR tag Found in the AWS Elastic Container Service console Repeat previous step for each profile Add, commit, and push changes to CodeCommit Navigate to the AWS CodePipeline console Click the -Pipeline pipeline Click the \"Release change\" button Click the \"Release\" button Monitor the pipeline as it builds Prime the Auto Scaling Group for each profile Navigate to the AWS EC2 console Select the \"Auto Scaling Groups\" sidebar link Select -cluster-AutoScalingGroup - Group details: Click the \"Edit\" button Desired capacity: Set to 1 Click the \"Update\" button Create a test notification Navigate to your notification calendar Create an event Set the event to last as long as you wish the notification to display The event title will appear as the notification title The description includes a metadata and message section Example: ``` profile: MY PROFILE, OTHER PROFILE type: info This is a notification 1. \\<meta> 1. profile: 1. Holds the name or names (comma separated) of the profiles where the notification will be displayed 1. type: 1. info 1. blue notification 1. success 1. green notification 1. warning 1. yellow notification 1. error 1. red notification 1. \\<message> 1. Your notification message 1. Sign up with your admin account, sign in, and add groups for each profile and sudo 1. Open the DeploymentURL in a web browser 1. Click the \"Sign in\" button 1. Click the \"Sign up\" link 1. Username: 1. The name used for the AdminUserName parameter of the CloudFormation stack 1. Name: 1. Your name 1. Email: 1. Enter the email address used for the AdminEmailAddress parameter in the -auth CloudFormation stack 1. Password: 1. A password 1. Click the \"Sign up\" button 1. Verification Code: 1. The verification code sent to your email address 1. Click the \"Confirm Account\" button 1. Add a group for each profile and for sudo 1. After confirming your account you should be redirected to the Server Options page 1. Click the \"Groups\" link at the top of the screen 1. Click the \"Add New Group\" button 1. Group Name: 1. The group name as it appears in the helm_config.yaml group_list 1. Note that this is not the display name and it contains underscores 1. Group Description: 1. (optional) Enter a group description 1. Group Type: 1. check \"action\" 1. This has no effect, but is useful for tracking user groups vs. profile groups 1. All Users?: 1. Check if you wish the profile to be accessible to all users 1. Is Enabled?: 1. check the box 1. Click the \"Add Group\" button 1. Repeat for all profiles 1. Repeat for a group named \"sudo\" 1. Do not enable sudo for all users! 1. This is useful for developers but avoid giving root privileges to regular users 1. Click the \"Home\" link at the top of the screen 1. Start up and test each profile 1. Click the \"Start My Server\" button 1. Select a profile 1. Click the \"Start\" button 1. Confirm that the profile runs as expected 1. Test notebooks as needed 1. Confirm that notifications appear 1. Repeat for each profile 1. Configure your local K8s config so you can manage your EKS cluster with kubectl 1. Add your AWS user to the trust relationship of the -cluster-access IAM role 1. Navigate to the AWS IAM console 1. Click the \"Roles\" link from the sidebar menu 1. Select the -cluster-access IAM role 1. Click the \"Trust relationships\" tab 1. Click the \"Edit trust relationship\" button 1. Add your AWS user ARN 1. Example json: 1. json { \"Version\": \"2008-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Principal\": { \"AWS\": [ \"arn:aws:iam::<account_#>:user/<username>\" ] }, \"Action\": \"sts:AssumeRole\" } ] } 1. Click the \"Update Trust Policy\" button 1. Add an AWS -cluster-access profile on your local machine 1. Example profile: 1. yaml [profile <deployment_name>-cluster-access] source_profile = your_source_profile region = your_region role_arn = arn:aws:iam::<AWS_account_id>:role/<deployment_name>-cluster-access 1. Run the helps/get_eks_kubeconfig.sh script in the opensarlab-cluster repo 1. Note: you will use this a lot and it may be helpful to create an alias in ~/.bash_aliases 1. Use kubectl Destroy Deployments At the end of a deployment's lifecycle, it is important to destroy it properly so no resources are left in place, costing you money. Deleting the -container, -auth, -cluster, and CloudFormation stacks will kill the deployment and remove its resources but there are a couple of prerequisite steps to prepare for proper deletion. Note: In the steps below, do not manually delete any S3 buckets after emptying them. If you delete the buckets, the CloudFormation stack deletions associated with those buckets will fail and you will have to recreate the empty buckets to proceed. Empty them and let CloudFormation delete them for you. Delete the -container CloudFormation stack Empty the codepipeline- - -container-container S3 bucket Navigate to the AWS S3 console Check the box next to the codepipeline- - -container-container S3 bucket Click the \"Empty\" button Confirm the deletion of bucket contents by typing \"permanently delete\" in the provided field Click the \"Empty\" button Delete ECR repos for each profile Navigate to the AWS Elastic Container Registry Click the box next to the / repository Click the \"Delete\" button Confirm the deletion by typing \"delete\" in the provided field Click the \"Delete\" button Repeat for each profile Delete the -container CloudFormation stack Navigate to the AWS CloudFormation console Click the box next to the -container stack Click the \"Delete\" button Click the \"Delete stack\" button Click the -container stack name Click the \"Events\" tab Monitor the stack deletion progress Click the refresh button periodically since the console doesn't update events automatically Delete the -auth CloudFormation stack Navigate to the AWS CloudFormation console Click the box next to the -auth stack Click the \"Delete\" button 1. Click the \"Delete stack\" button Click the -auth stack name Click the \"Events\" tab Monitor the stack deletion progress Click the refresh button periodically since the console doesn't update events automatically Delete the -cluster CloudFormation stack Navigate to the AWS CloudFormation console Click the box next to the -cluster stack Click the \"Delete\" button 1. Click the \"Delete stack\" button Click the -cluster stack name Click the \"Events\" tab Monitor the stack deletion progress Click the refresh button periodically since the console doesn't update events automatically Delete the CloudFormation stack Delete hub and notifications ECR repos Navigate to the AWS Elastic Container Registry Click the box next to the /hub repository Click the \"Delete\" button Confirm the deletion by typing \"delete\" in the provided field Click the \"Delete\" button Repeat the above steps for the /notifications repository Empty the codepipeline- - S3 bucket Navigate to the AWS S3 console Check the box next to the codepipeline- - S3 bucket Click the \"Empty\" button Confirm the deletion of bucket contents by typing \"permanently delete\" in the provided field Click the \"Empty\" button Delete the CloudFormation stack Navigate to the AWS CloudFormation console Click the box next to the stack Click the \"Delete\" button 1. Click the \"Delete stack\" button Click the stack name Click the \"Events\" tab Monitor the stack deletion progress Click the refresh button periodically since the console doesn't update events automatically Delete EBS snapshots Navigate to the AWS EC2 console Click the \"Snapshots\" link in the sidebar menu Filter by osl-stackname: Double check that you filtered for the correct deployment! Select all snapshots Select \"Delete\" from the \"Actions\" menu Confirm by clicking the \"Yes, delete\" button Delete EBS volumes Navigate to the AWS EC2 console Click the \"Volumes\" link in the sidebar menu Filter by osl-stackname: Double check that you filtered for the correct deployment! Select all volumes Select \"Delete volumes\" from the \"Actions\" menu Confirm by clicking the \"Yes, delete\" button (Optional) Delete the -cluster and -container CodeCommit repositories CodeCommit repos are cheap If you think you may re-deploy the same deployment, you may want to ease future work by leaving these repos in place You could also download a zip of your repos, store them in S3, and then delete them Navigate to the AWS CodeCommit console Check the box next to the -container repo Click the \"Delete repository\" button Confirm the deletion by typing \"delete\" in the provided field Click the \"Delete\" button Repeat the above steps for the -cluster repo Confirm that all resources have been deleted Wait a day for deleted resources to update in the tag editor Navigate to the AWS Resource Groups and Tag Editor console Select the \"Tag Editor\" link in the sidebar menu Tags: Key: Cost allocation tag Value: Click the \"Search resources\" button Identify and delete any remaining resources","title":"Deploy OpenSARlab to AWS"},{"location":"dev-guides/deploy_OpenSARlab/#deploy-opensarlab-to-an-aws-account","text":"A note about deployments: A deployment of OpenSARlab refers to a standalone instance of OpenSARlab. If you are setting up OpenSARlab for several classes and/or collaborative groups with disparate needs or funding sources, it may be useful to give them each their own standalone deployment. This separates user group authentication, simplifies billing for each group, and allows for easy cleanup at the end of a project or class (just delete the deployment). In the following instructions, replace any occurrence of \" \" with the deployment name you have chosen. Make your deployment name lowercase and use no special characters other than dashes (-). It will be used to generate part of the Cognito callback URL and CloudFormation stack names also follow the same naming convention.","title":"Deploy OpenSARlab to an AWS account"},{"location":"dev-guides/deploy_OpenSARlab/#take-aws-ses-out-of-sandbox","text":"The AWS Simple Email Service is used by OpenSARlab to send emails to users and administrators. These include authentication related notifications and storage lifecycle management messages. While SES is in sandbox, you are limited to sending 1 email per second with no more than 200 in a 24 hour period, and they may only be sent from an SES verified address to other SES verified addresses. Note: Provide a detailed explanation of your SES use and email policies when applying to exit the sandbox or you will be denied. Approval can take 24-48 hours Follow these instructions to take your SES out of sandbox.","title":"Take AWS SES out of sandbox"},{"location":"dev-guides/deploy_OpenSARlab/#create-an-aws-cost-allocation-tag","text":"Note: only management accounts can create cost allocation tags Create a cost allocation tag or have one created by someone with access Give it an available name that makes sense for tracking deployment names associated with AWS resources i.e. \"deployment_name\"","title":"Create an AWS Cost Allocation Tag"},{"location":"dev-guides/deploy_OpenSARlab/#add-dockerhub-credentials-to-aws-secrets-manager","text":"This deployment uses a few publicly available docker images. Due to dockerhub rate limits ( https://www.docker.com/increase-rate-limits ), you will need to set up a dockerhub account. A free-tier account will suffice. CodePipeline's ip address is shared by many users and you will likely hit the rate limit as an anonymous user ( details here ). Note: By default this secret will be used for multiple deployments. Optionally, you could edit the codebuild section in the cf-cluster.yml to point to a different secret. If you don't have a dockerhub account, create one here Open the AWS Secrets Manager console Click the \"Store a new secret\" button Page 1: Select \"Other type of secrets\" Select the \"Plaintext\" tab Delete the default content Add your username and password, separated by a space Example: username password Click the \"Next\" button Page 2: Secret name dockerhub/creds Click the \"Next\" button Page 3: Click the \"Next\" button Page 4: Click the \"Store\" button","title":"Add dockerhub credentials to AWS Secrets Manager"},{"location":"dev-guides/deploy_OpenSARlab/#setup-an-ical-calendar-for-notifications","text":"Notifications are generated from iCal calendar events. ASF uses Google Calendar but any publicly accessible iCal formatted calendar should work as well Create a public iCal formatted calendar The iCal formatted url will be needed in later Notification calendar events must be properly formatted. Formatting details available in the Take care of odds and ends section","title":"Setup an iCal calendar for notifications"},{"location":"dev-guides/deploy_OpenSARlab/#store-your-ca-certificate","text":"OpenSARlab will lack full functionality if not using https (SSL certification) Follow these instructions to import your CA certificate into the AWS Certificate Manager","title":"Store your CA certificate"},{"location":"dev-guides/deploy_OpenSARlab/#prepare-codecommit-repos","text":"TODO Do this differently All the public OpenSARlab repos are in the ASFOpenSARlab Github Org Create a -container CodeCommit repo in your AWS account Create a -cluster CodeCommit repo Clone the -container and -cluster repos to your local computer using ssh cd into your local -container repo add ASFOpenSARlab/opensarlab-container as a remote on your local -container repo git remote add github https://github.com/ASFOpenSARlab/opensarlab-container.git Pull the remote opensarlab-container repo into your local -container repo git pull github main Create a main branch in the -container repo git checkout -b main Push to the remote -container repo git push origin main cd into your local -cluster repo add ASFOpenSARlab/opensarlab-cluster as a remote on your local -cluster repo git remote add github https://github.com/ASFOpenSARlab/opensarlab-cluster.git Pull the remote opensarlab-cluster repo into your local -cluster repo git pull github main Create a main branch in the -cluster repo git checkout -b main Push to the remote -cluster repo git push origin main You should now have container and cluster repos in CodeCommit that are duplicates of those found in ASFOpenSARlab","title":"Prepare CodeCommit Repos"},{"location":"dev-guides/deploy_OpenSARlab/#create-an-s3-bucket-to-hold-the-lambda-handler-script","text":"Create an S3 bucket in your AWS account called -lambda Alternatively, you could use the same bucket to hold lambdas for multiple deployments","title":"Create an S3 bucket to hold the lambda handler script"},{"location":"dev-guides/deploy_OpenSARlab/#customize-opensarlab_container-code-for-deployment","text":"The opensarlab-container repo contains one example image named \"sar\", which you can reference when creating new images. Images can be used by multiple profiles Note: It is easiest to work in your local repo and push your changes when you're done. Duplicate the images/sar directory and rename it, using your chosen image name Create and add any additional custom jupyter magic commands to the jupyter-hooks/custom_magics directory Add any additional scripts you may have created for use in your image to the scripts directory Add any image test scripts to tests directory Edit images/ /jupyter-hooks/sar.sh Rename sar.sh to .sh Copy any additional custom Jupyter magic scripts to $HOME/.ipython/image_default/startup/ (alongside 00-df.py) Edit the repos being pulled to suit your deployment and image needs Edit images/ /build.sh Change 6 $DOCKER_REGISTRY namespaces from \"sar\" to your image namespace Repeat the previous step, adding scripts for any additional images Edit cf-container.yaml SARRepository Change the resource name (SARRepository) so it makes sense given the image that will use it Change the \"/sar\" portion of RepositoryName to use a namespace matching your image name Pipeline Name: !Sub ${AWS::StackName}-Build-Images Change \"- Name: images-sar\" to \"- Name: images- \" EnvironmentVariables Change the IMAGE_NAME value from \"sar\" to \" Add stages for any additional images Edit dockerfile Adjust the packages in the 2nd apt install command to suit your image needs Add any pip packages you wish installed in the base conda environment Add any conda packages you wish installed in the base conda environment Create any conda environments you would like pre-installed before \"USER jovyan\" If using environment.yml files, store them in an \"envs\" directory in /jupyter-hooks, and they will be copied into the container RUN conda env create -f /etc/jupyter-hooks/envs/ _env.yml --prefix /etc/jupyter-hooks/envs/ Run any tests for this image that you added to the tests directory Remove the images/sar directory and sar.sh test script, unless you plan to use the sar image Add a test script for your image use sar.sh as an example name it .sh Edit tests/finale.sh Delete \"bash sar.sh\" if not using it Add \"bash .sh\" Add, commit, and push changes to the remote CodeCommit repo","title":"Customize opensarlab_container code for deployment"},{"location":"dev-guides/deploy_OpenSARlab/#customize-opensarlab_cluster-code-for-deployment","text":"The opensarlab-cluster repo contains TODO comments everywhere deployment specific edits should be made Note: Most IDEs have functionality to easily locate and organize TODOs. Searching the code for \"TODO\" will also work. hub/dockerfile Add your organization's name as MAINTAINER Adjust the images (jpgs, pngs, etc...) and templates being copied to the docker image hub/etc/jupyterhub/custom/delete_snapshot.py Edit admin email addresses (2 locations) hub/usr/local/share/jupyterhub/templates/login.html Edit the images and messages that appear on the login page hub/usr/local/share/jupyterhub/templates/pending.html Edit the message to users that their account is pending approval notifications/dock Add the ICAL_URL of your notification calendar notifications/dockerfile Add your organization's name as MAINTAINER cf-cluster.yaml Add a NodeInstanceType parameter for every EC2 type must be alphanumeric Remove the example NodeInstanceTypePROFILE1 resource Add a LaunchConfiguration for every NodeInstanceType must be alphanumeric InstanceType should match the NodeInstanceType created above The server_type in UserData must match profile's server_type that you will use in helm_config.yaml Add an AutoScalingGroup resource for every NodeInstanceType must be alphanumeric LaunchConfigurationName should match new LaunchConfiguration created above Remove the example AutoScalingGroupPROFILE1 resource Remove the example LaunchConfigurationPROFILE1 resource helm_config.yaml Add new profiles, using the example PROFILE_1 as a template Reference the kubespawner docs for more options and details Change the name of the profile being search for in group_list Change the display_name Change the profile description Change the extra_labels and node_selector server_types to match the server_type used in the profiles LaunchConfiguration in cf-cluster.yaml Adjust the path to the postStart lifecycle hook Adjust the kubespawner_override args --NotebookApp.jinja_template_vars PROFILE_NAME to the correct profile name Adjust the mem_limit The maximum amount of memory available to each user <= memory available for EC2 type Adjust the mem_guarantee (or cpu_guarantee) The minimum amount of memory guaranteed to each user If there is not enough memory on any existing node, the autoscaler will spin up a new node Use the mem_guarantee to determine how nodes should be shared among users Even if not sharing nodes, do not guarantee all available memory The node requires some memory for setup (varies and may take some trial and error to figure out how much to reserve) Adjust the cpu_guarantee (or mem_guarantee) The minimum EC2 cpu units guaranteed to each user If there aren't enough cpu units left on a node for the next user, the autoscaler will spin up a new node Use the cpu_guarantee to determine how nodes should be shared among users Even if not sharing nodes, do not guarantee all available cpus The node requires some memory for setup Adjust the storage capacity This should match the storage capacity used for all profiles You can increase volume sizes at a later date Reducing volume sizes is not advised due to a high likelihood of data loss Remove the example PROFILE_1 lambda_handler.py Lambdas are used by Cognito event triggers for logging and emailing notifications to users and administrators Create a lambda_handler.py file based on lambda_handler.py.example Adjust email messages to suit the needs of the deployment zip the file, creating lambda_handler.py.zip be sure to keep the \".py\" as part of the filename Upload the zip to the -lambda S3 bucket After setting up Cognito for the first time, anytime you make changes to this file you will need to: Change the name of the zip file Do not change the name of the lambda_handler.py file it contains Upload it to the lambda S3 bucket Update the EmailLambdaKeyName parameter in the cognito CloudFormation template to match the new filename After updating the pipeline, set all Cognito triggers to 'None', save them, set them back to the correct lambdas, and save them again Add, commit, and push changes to the remote CodeCommit repo","title":"Customize opensarlab_cluster code for deployment"},{"location":"dev-guides/deploy_OpenSARlab/#build-the-cognito-cloudformation-stack","text":"Open CloudFormation in the AWS console Click the \"Create stack\" button and select \"With new resources (standard)\" Page 1 : Create stack Under \"Specify template\", check \"Upload a template file\" Use the file chooser to select cf-cognito.py from your local branch of the -cluster repo Click the \"Next\" button Page 2: Specify stack details Stack name: -auth dashes (-) are the only allowed special characters lowercase only AdminEmailAddress: SES verified email address of the primary administrator This is the sender address users will see on confirmation, verification, and volume lifecycle emails AdminEmailSNSArn: Arn of the above admin email address Must be AWS SES verified (easy to do in the Amazon Simple Email Service console) DeploymentURL Enter the deployment domain, if known (i.e. https://deployment_name.your_domain.tdl) The placeholder domain can be left in place temporarily if the actual domain is not yet known CostTagValue EmailLambdaBucketName -lambda EmailLambdaKeyName lambda_handler.py.zip (or whatever you named it) Click the \"Next\" button Page 3: Configure stack options Tags: Key: Cost allocation tag Value: Click the \"Next\" button Page 4: Review -auth Review and confirm correctness Check the box next to \"I acknowledge that AWS CloudFormation might create IAM resources\" Click the \"Create Stack Button\" Monitor the stack build for errors and rollbacks The screen does not self-update Use the refresh buttons If the build fails and rolls back goto the CloudFormation stacks page select and delete the failed stack before correcting any errors and trying again","title":"Build the Cognito CloudFormation stack"},{"location":"dev-guides/deploy_OpenSARlab/#build-the-container-cloudformation-stack","text":"This will create the hub image, images for each profile, and store them in namespaced ECR repos Open CloudFormation in the AWS console Click the \"Create stack\" button and select \"With new resources (standard)\" Page 1 : Create stack Under \"Specify template\", check \"Upload a template file\" Use the file chooser to select cf-container.py from your local branch of the -container repo Click the \"Next\" button Page 2: Specify stack details Stack name: -container CodeCommitSourceBranch: The name of the production branch of the -container CodeCommit repo CodeCommitSourceRepo: -container CostTagValue Page 3: Configure stack options Tags: Key: Cost allocation tag Value: Click the \"Next\" button Page 4: Review -auth Review and confirm correctness Check the box next to \"I acknowledge that AWS CloudFormation might create IAM resources\" Click the \"Create Stack Button\" Monitor the stack build for errors and rollbacks The screen does not self-update Use the refresh buttons If the build fails and rolls back goto the CloudFormation stacks page select and delete the failed stack before correcting any errors and trying again When stack creation is complete, open CodePipeline in the AWS console Open the -container-Container-Pipeline pipeline and monitor it as it runs Click the \"details\" link under each stage action for a closer inspection If part of the pipeline fails Identify the issue from the \"details\" output Correct the code Update the -container repo Go to the pipeline Click the \"Release change\" button","title":"Build the container CloudFormation stack"},{"location":"dev-guides/deploy_OpenSARlab/#build-the-cluster-cloudformation-stack","text":"This CloudFormation stack dynamically creates a second CloudFormation stack. You will end up with a stack and a -cluster stack. Open CloudFormation in the AWS console Page 1 : Create stack Click the \"Create stack\" button and select \"With new resources (standard)\" Under \"Specify template\", check \"Upload a template file\" Use the file chooser to select cf-pipeline.py from your local branch of the repo Click the \"Next\" button Page 2: Specify stack details Stack name: -cluster AdminUserName: JupyterHub Admin username Initial default admin with access to the JupyterHub admin and group pages CertificateArn: Arn associated with the CA certificate you stored in AWS Certificate Manager arn:aws:acm: : :certificate/ CodeCommitRepoName: Name of the CodeCommit repo holding your -cluster code CodeCommitBranchName: Name of the branch holding this deployment's cluster code ContainerNamespace: -container CostTagKey: Cost allocation tag CostTagValue: ICALUrl: The iCal formatted URL of the calendar used for notifications DeploymentURL: Your custom URL (should match DeploymentURL parameter in -auth stack) If left blank, the default load balancer will be used Can be updated later OAuthPoolName: -auth Page 3: Configure stack options Tags: Key: Cost allocation tag Value: Click the \"Next\" button Page 4: Review -auth Review and confirm correctness Check the box next to \"I acknowledge that AWS CloudFormation might create IAM resources\" Click the \"Create Stack\" button Open CodePipeline in the AWS console Open the -Pipeline pipeline and monitor it as it runs Click the \"details\" link under each stage action for a closer inspection","title":"Build the cluster CloudFormation stack"},{"location":"dev-guides/deploy_OpenSARlab/#take-care-of-odds-and-ends","text":"Update the DeploymentURL parameters in 2 CloudFormation stacks with load balancer URL or your custom URL If you already entered a custom URL when building the stacks, skip this step If using the load balancer URL Find it in the 'Outputs' tab of the CloudFormation console for the -cluster CloudFormation stack Note: you will have to prepend \"https://\" to it Update the stack Navigate to the stack CloudFormation console Click the 'Update' button Page 1: Leave 'Use current template' selected Click the 'Next' button Page 2: Update the DeploymentURL parameter Click the 'Next' button Page 3: Click the 'Next' button Page 4: Review and confirm correctness Check the box next to \"I acknowledge that AWS CloudFormation might create IAM resources\" Click the \"Update Stack\" button Repeat the above process for the -auth stack Add the cost allocation tag to the EKS cluster Navigate to the AWS EKS console click the \"Clusters\" link in the sidebar menu Click on -cluster Click the \"Tags\" tab Click the \"Manage tags\" button Click the \"Add tag\" button Key: Cost allocation tag Value: Wrap things up in Cognito Navigate to the AWS Cognito console Click the \"Manage User Pools\" button Click the \" -auth\" user pool button Add the cost allocation tag to the Cognito user pool Click the \"Tags\" sidebar menu link Click the \"Add tag\" link Tag Key: Cost allocation tag Tag Value: Click the \"Save changes\" button Reset triggers Cognito triggers can fall out of sync with email and logging lambdas so it is safest to reset them Click the \"Triggers\" sidebar menu link Set all lambda functions to \"none\" Click the \"Save changes\" button Click the \"Triggers\" sidebar menu link Pre sign-up: select the -auth-LogFunction- lambda Pre authentication: select the -auth-PreAuthFunction- lambda Post authentication: select the -auth-LogFunction- lambda Post confirmation: select the -auth-PostEmailVerificationFun- lambda Pre Token Generator: select the -auth-LogFunction- lambda Leave the remaining triggers set to \"none\" Click the \"Save changes\" button Update image tags for every profile in helm_config.yml ( -cluster repository) hub:image:tag 'latest' will only work for the hub:image:tag on the first build It is safest to never use the \"latest\" tag Use the image's ECR timestamp Found in the AWS Elastic Container Service console profile image_url tag update the tag at the end of the ECR image_url Use the image's 7 character long ECR tag Found in the AWS Elastic Container Service console Repeat previous step for each profile Add, commit, and push changes to CodeCommit Navigate to the AWS CodePipeline console Click the -Pipeline pipeline Click the \"Release change\" button Click the \"Release\" button Monitor the pipeline as it builds Prime the Auto Scaling Group for each profile Navigate to the AWS EC2 console Select the \"Auto Scaling Groups\" sidebar link Select -cluster-AutoScalingGroup - Group details: Click the \"Edit\" button Desired capacity: Set to 1 Click the \"Update\" button Create a test notification Navigate to your notification calendar Create an event Set the event to last as long as you wish the notification to display The event title will appear as the notification title The description includes a metadata and message section Example: ``` profile: MY PROFILE, OTHER PROFILE type: info This is a notification 1. \\<meta> 1. profile: 1. Holds the name or names (comma separated) of the profiles where the notification will be displayed 1. type: 1. info 1. blue notification 1. success 1. green notification 1. warning 1. yellow notification 1. error 1. red notification 1. \\<message> 1. Your notification message 1. Sign up with your admin account, sign in, and add groups for each profile and sudo 1. Open the DeploymentURL in a web browser 1. Click the \"Sign in\" button 1. Click the \"Sign up\" link 1. Username: 1. The name used for the AdminUserName parameter of the CloudFormation stack 1. Name: 1. Your name 1. Email: 1. Enter the email address used for the AdminEmailAddress parameter in the -auth CloudFormation stack 1. Password: 1. A password 1. Click the \"Sign up\" button 1. Verification Code: 1. The verification code sent to your email address 1. Click the \"Confirm Account\" button 1. Add a group for each profile and for sudo 1. After confirming your account you should be redirected to the Server Options page 1. Click the \"Groups\" link at the top of the screen 1. Click the \"Add New Group\" button 1. Group Name: 1. The group name as it appears in the helm_config.yaml group_list 1. Note that this is not the display name and it contains underscores 1. Group Description: 1. (optional) Enter a group description 1. Group Type: 1. check \"action\" 1. This has no effect, but is useful for tracking user groups vs. profile groups 1. All Users?: 1. Check if you wish the profile to be accessible to all users 1. Is Enabled?: 1. check the box 1. Click the \"Add Group\" button 1. Repeat for all profiles 1. Repeat for a group named \"sudo\" 1. Do not enable sudo for all users! 1. This is useful for developers but avoid giving root privileges to regular users 1. Click the \"Home\" link at the top of the screen 1. Start up and test each profile 1. Click the \"Start My Server\" button 1. Select a profile 1. Click the \"Start\" button 1. Confirm that the profile runs as expected 1. Test notebooks as needed 1. Confirm that notifications appear 1. Repeat for each profile 1. Configure your local K8s config so you can manage your EKS cluster with kubectl 1. Add your AWS user to the trust relationship of the -cluster-access IAM role 1. Navigate to the AWS IAM console 1. Click the \"Roles\" link from the sidebar menu 1. Select the -cluster-access IAM role 1. Click the \"Trust relationships\" tab 1. Click the \"Edit trust relationship\" button 1. Add your AWS user ARN 1. Example json: 1. json { \"Version\": \"2008-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Principal\": { \"AWS\": [ \"arn:aws:iam::<account_#>:user/<username>\" ] }, \"Action\": \"sts:AssumeRole\" } ] } 1. Click the \"Update Trust Policy\" button 1. Add an AWS -cluster-access profile on your local machine 1. Example profile: 1. yaml [profile <deployment_name>-cluster-access] source_profile = your_source_profile region = your_region role_arn = arn:aws:iam::<AWS_account_id>:role/<deployment_name>-cluster-access 1. Run the helps/get_eks_kubeconfig.sh script in the opensarlab-cluster repo 1. Note: you will use this a lot and it may be helpful to create an alias in ~/.bash_aliases 1. Use kubectl","title":"Take care of odds and ends"},{"location":"dev-guides/deploy_OpenSARlab/#destroy-deployments","text":"At the end of a deployment's lifecycle, it is important to destroy it properly so no resources are left in place, costing you money. Deleting the -container, -auth, -cluster, and CloudFormation stacks will kill the deployment and remove its resources but there are a couple of prerequisite steps to prepare for proper deletion. Note: In the steps below, do not manually delete any S3 buckets after emptying them. If you delete the buckets, the CloudFormation stack deletions associated with those buckets will fail and you will have to recreate the empty buckets to proceed. Empty them and let CloudFormation delete them for you. Delete the -container CloudFormation stack Empty the codepipeline- - -container-container S3 bucket Navigate to the AWS S3 console Check the box next to the codepipeline- - -container-container S3 bucket Click the \"Empty\" button Confirm the deletion of bucket contents by typing \"permanently delete\" in the provided field Click the \"Empty\" button Delete ECR repos for each profile Navigate to the AWS Elastic Container Registry Click the box next to the / repository Click the \"Delete\" button Confirm the deletion by typing \"delete\" in the provided field Click the \"Delete\" button Repeat for each profile Delete the -container CloudFormation stack Navigate to the AWS CloudFormation console Click the box next to the -container stack Click the \"Delete\" button Click the \"Delete stack\" button Click the -container stack name Click the \"Events\" tab Monitor the stack deletion progress Click the refresh button periodically since the console doesn't update events automatically Delete the -auth CloudFormation stack Navigate to the AWS CloudFormation console Click the box next to the -auth stack Click the \"Delete\" button 1. Click the \"Delete stack\" button Click the -auth stack name Click the \"Events\" tab Monitor the stack deletion progress Click the refresh button periodically since the console doesn't update events automatically Delete the -cluster CloudFormation stack Navigate to the AWS CloudFormation console Click the box next to the -cluster stack Click the \"Delete\" button 1. Click the \"Delete stack\" button Click the -cluster stack name Click the \"Events\" tab Monitor the stack deletion progress Click the refresh button periodically since the console doesn't update events automatically Delete the CloudFormation stack Delete hub and notifications ECR repos Navigate to the AWS Elastic Container Registry Click the box next to the /hub repository Click the \"Delete\" button Confirm the deletion by typing \"delete\" in the provided field Click the \"Delete\" button Repeat the above steps for the /notifications repository Empty the codepipeline- - S3 bucket Navigate to the AWS S3 console Check the box next to the codepipeline- - S3 bucket Click the \"Empty\" button Confirm the deletion of bucket contents by typing \"permanently delete\" in the provided field Click the \"Empty\" button Delete the CloudFormation stack Navigate to the AWS CloudFormation console Click the box next to the stack Click the \"Delete\" button 1. Click the \"Delete stack\" button Click the stack name Click the \"Events\" tab Monitor the stack deletion progress Click the refresh button periodically since the console doesn't update events automatically Delete EBS snapshots Navigate to the AWS EC2 console Click the \"Snapshots\" link in the sidebar menu Filter by osl-stackname: Double check that you filtered for the correct deployment! Select all snapshots Select \"Delete\" from the \"Actions\" menu Confirm by clicking the \"Yes, delete\" button Delete EBS volumes Navigate to the AWS EC2 console Click the \"Volumes\" link in the sidebar menu Filter by osl-stackname: Double check that you filtered for the correct deployment! Select all volumes Select \"Delete volumes\" from the \"Actions\" menu Confirm by clicking the \"Yes, delete\" button (Optional) Delete the -cluster and -container CodeCommit repositories CodeCommit repos are cheap If you think you may re-deploy the same deployment, you may want to ease future work by leaving these repos in place You could also download a zip of your repos, store them in S3, and then delete them Navigate to the AWS CodeCommit console Check the box next to the -container repo Click the \"Delete repository\" button Confirm the deletion by typing \"delete\" in the provided field Click the \"Delete\" button Repeat the above steps for the -cluster repo Confirm that all resources have been deleted Wait a day for deleted resources to update in the tag editor Navigate to the AWS Resource Groups and Tag Editor console Select the \"Tag Editor\" link in the sidebar menu Tags: Key: Cost allocation tag Value: Click the \"Search resources\" button Identify and delete any remaining resources","title":"Destroy Deployments"},{"location":"dev-guides/system_diagram/","text":"Return to Table of Contents OpenSARlab System Diagram","title":"System diagram"},{"location":"dev-guides/system_diagram/#opensarlab-system-diagram","text":"","title":"OpenSARlab System Diagram"},{"location":"dev-guides/whitelist/","text":"Setup a Google Sheet whitelist and a service account 1) goto console.cloud.google.com 1) If you don't have a project yet, click on the project button at the top left of the screen 1) Click the \"New Project\" button in th pop-up 1) create your project 1) Select API & Services from the left menu 1) Select \"Dashboard\" 1) Click the \"+ ENABLE APIS AND SERVICES\" button at the top of the screen 1) Search for \"Google Sheets API\" 1) Enable it 1) Search for \"Google Drive API\" 1) Enable it 1) From the top left main menu (3 horizontal bars), select \"IAM & Admin\" 1) \"Service Accounts\" 1) Click the \"+ CREATE SERVICE ACCOUNT\" button at the top of the screen 1) Service account name 1) choose a name 1) Click \"Continue\" 1) Grant this service account access to project 1) Set the role to basic -> \"viewer\" 1) Continue 1) Grant users access to this service account 1) skip this 1) Click the \"Done\" button 1) Click on the service account you just created 1) Select the \"KEYS\" tab 1) Select \"Create new key\" from the \"ADD KEY\" dropdown 1) Select json 1) Click the \"CREATE\" button 1) A json credentials file will be automatically downloaded 1) This is sensitive information and should be kept confidential 1) Move it somewhere 1) Navigate back to the service account details page 1) copy the service account email to your clipboard 1) Navigate to Google Sheets 1) Create a spreadsheet to hold your whitelist 1) Don't make it public as it will contain sensitive information (names, usernames, email addresses) 1) Share the sheet with the service account email address as you would share a sheet with a typical human user 1) Hit the share button and paste in the address Extra Notes The Google Sheet should follow the naming convention <deployment_name>-whitelist The first sheet must contain the whitelist The first column must contain user emails There are no restrictions on the use of additional columns and/or sheets ASF will have access to the sheet so it is recommended that you not include sensitive information","title":"Setup a Google Sheet whitelist and a service account"},{"location":"dev-guides/whitelist/#setup-a-google-sheet-whitelist-and-a-service-account","text":"1) goto console.cloud.google.com 1) If you don't have a project yet, click on the project button at the top left of the screen 1) Click the \"New Project\" button in th pop-up 1) create your project 1) Select API & Services from the left menu 1) Select \"Dashboard\" 1) Click the \"+ ENABLE APIS AND SERVICES\" button at the top of the screen 1) Search for \"Google Sheets API\" 1) Enable it 1) Search for \"Google Drive API\" 1) Enable it 1) From the top left main menu (3 horizontal bars), select \"IAM & Admin\" 1) \"Service Accounts\" 1) Click the \"+ CREATE SERVICE ACCOUNT\" button at the top of the screen 1) Service account name 1) choose a name 1) Click \"Continue\" 1) Grant this service account access to project 1) Set the role to basic -> \"viewer\" 1) Continue 1) Grant users access to this service account 1) skip this 1) Click the \"Done\" button 1) Click on the service account you just created 1) Select the \"KEYS\" tab 1) Select \"Create new key\" from the \"ADD KEY\" dropdown 1) Select json 1) Click the \"CREATE\" button 1) A json credentials file will be automatically downloaded 1) This is sensitive information and should be kept confidential 1) Move it somewhere 1) Navigate back to the service account details page 1) copy the service account email to your clipboard 1) Navigate to Google Sheets 1) Create a spreadsheet to hold your whitelist 1) Don't make it public as it will contain sensitive information (names, usernames, email addresses) 1) Share the sheet with the service account email address as you would share a sheet with a typical human user 1) Hit the share button and paste in the address","title":"Setup a Google Sheet whitelist and a service account"},{"location":"dev-guides/whitelist/#extra-notes","text":"The Google Sheet should follow the naming convention <deployment_name>-whitelist The first sheet must contain the whitelist The first column must contain user emails There are no restrictions on the use of additional columns and/or sheets ASF will have access to the sheet so it is recommended that you not include sensitive information","title":"Extra Notes"},{"location":"release-notes/release_06-2021/","text":"Welcome to the June 2021 OpenSARlab Upgrade! Changes: conda environments (BREAKING CHANGE ALERT: please read details below) nbgitpuller patch installed jupyter-resource-usage profile identifier Conda Environments What is conda and what are conda environments? Conda is an open-source package and environment manager. It identifies and attempts to handle dependency related issues when installing multiple software packages. Users create conda environments, in which multiple software packages may be installed. This allows a user to setup a variety of environments, each containing an assortment of software suited to a particular use-case. If a user needs to install software that would conflict with a previously installed package, they can create a new environment in which to install it and avoid the conflict. They can then switch between environments to handle various use-cases. How did OpenSARlab use conda previously? OpenSARlab previously had conda installed but it was only used as a package manager. Conda was not initialized. What problems did this cause? Not initializing conda made it difficult for users to create and use conda environments effectively All notebooks ran in the same environment, which involved a delicate balance of software installations, making the OpenSARlab docker image very brittle All user accounts had every package installed regardless of individual need, making the OpenSARlab docker image unnecessarily large and slow to build Changes to the conda environment made by users did not persist after server shutdowns Packages such as ISCE, MintPY, TRAIN, and ARIA-Tools were installed in an area to which users lacked access, making updates and development of those packages difficult What has changed? All notebooks now run in one of 5 conda environments, each suited different use-cases rtc_analysis insar_analysis machine_learning hydrosar Python 3 (the base conda environment containing minimal software) Conda environments are stored in /home/jovyan/.local/envs this location is on the user volume, so changes persist after server restarts New OpenSARlab users are prompted to select the environments they would like pre-built for them when signing up for an account unselected environments may always be added later Current users must build their own environments using the provided notebook and accompanying environment.yml files Python kernels from the appropriate environments have been pre-selected for all notebooks and saved in their metadata if the needed environment doesn't yet exist, users will be prompted to change the kernel to one that does Note that an incorrect environment will likely be missing needed software and be incapable of running a notebook for which it was not intended Instead, create the needed environment using this notebook Code has been added to each notebook to check that it is running in the correct environment Warnings explain how to change to the correct environment if it has been created but the notebook isn't using it Warnings direct users to a notebook to create the environment if it does not yet exist There is also a minimal environment called \"scratch\" that is intended for user adaptation and experimentation This has been added for quick and easy access but users may add as many custom environments as they like What will happen if I don't create any new environments? You will encounter environment warnings in the notebooks. You will not have access to the software needed to run the notebooks, which will trigger errors (ModuleNotFoundError). I don't want to wait for the notebooks to yell at me and give me environment warnings. How can I create the environments I'd like right now? Good choice! Head over to Create_OSL_Conda_Environments.ipynb and run the notebook. You will be prompted to select an environment from a list of options. Rerun the notebook for every environment you wish to add. nbgitpuller Patch What is nbgitpuller? nbgitpuller performs automatic merging of git repositories in a class-like setting. It handles merging in situations where instructor provided files may be edited both by students and/or the instructor. Its goal is to pull in an instructor's changes while preserving any edits a student has made. Where a conflict exists, it saves the student altered file with a timestamp appended to the filename and pulls in the instructor update. What was wrong with nbgitpuller? nbgitpuller did not successfully handle all scenarios encountered in OpenSARlab. When it failed, users were locked out of their accounts. They had to login using a purpose-built profile that would skip the nbgitpuller. They could then identify and manually rectify the git state that caused the nbgitpuller to fail. How was this issue addressed? We have populated user accounts with an altered version of nbgitpuller/pull.py - The nbgitpuller will no longer attempt to checkout files that have been removed from a remote branch - If the user has changed to a branch other than main in the opensarlab-notebooks git repository, the merge will be aborted - Users can still merge other branches from the command line Installed jupyter-resource-usage What is jupyter-resource-usage? jupyter-resource-usage is an extension that displays how much memory a notebook server is using. The information is displayed at the top-right of every running Jupyter Notebook. It indicates the total memory used by all running notebooks, kernels, terminals, etc. Added profile identifier The name of the current profile now appears to the left of the \"Logout\" button on the home page.","title":"June 2021"},{"location":"release-notes/release_06-2021/#welcome-to-the-june-2021-opensarlab-upgrade","text":"","title":"Welcome to the June 2021 OpenSARlab Upgrade!"},{"location":"release-notes/release_06-2021/#changes","text":"conda environments (BREAKING CHANGE ALERT: please read details below) nbgitpuller patch installed jupyter-resource-usage profile identifier","title":"Changes:"},{"location":"release-notes/release_06-2021/#conda-environments","text":"","title":"Conda Environments"},{"location":"release-notes/release_06-2021/#what-is-conda-and-what-are-conda-environments","text":"Conda is an open-source package and environment manager. It identifies and attempts to handle dependency related issues when installing multiple software packages. Users create conda environments, in which multiple software packages may be installed. This allows a user to setup a variety of environments, each containing an assortment of software suited to a particular use-case. If a user needs to install software that would conflict with a previously installed package, they can create a new environment in which to install it and avoid the conflict. They can then switch between environments to handle various use-cases.","title":"What is conda and what are conda environments?"},{"location":"release-notes/release_06-2021/#how-did-opensarlab-use-conda-previously","text":"OpenSARlab previously had conda installed but it was only used as a package manager. Conda was not initialized.","title":"How did OpenSARlab use conda previously?"},{"location":"release-notes/release_06-2021/#what-problems-did-this-cause","text":"Not initializing conda made it difficult for users to create and use conda environments effectively All notebooks ran in the same environment, which involved a delicate balance of software installations, making the OpenSARlab docker image very brittle All user accounts had every package installed regardless of individual need, making the OpenSARlab docker image unnecessarily large and slow to build Changes to the conda environment made by users did not persist after server shutdowns Packages such as ISCE, MintPY, TRAIN, and ARIA-Tools were installed in an area to which users lacked access, making updates and development of those packages difficult","title":"What problems did this cause?"},{"location":"release-notes/release_06-2021/#what-has-changed","text":"All notebooks now run in one of 5 conda environments, each suited different use-cases rtc_analysis insar_analysis machine_learning hydrosar Python 3 (the base conda environment containing minimal software) Conda environments are stored in /home/jovyan/.local/envs this location is on the user volume, so changes persist after server restarts New OpenSARlab users are prompted to select the environments they would like pre-built for them when signing up for an account unselected environments may always be added later Current users must build their own environments using the provided notebook and accompanying environment.yml files Python kernels from the appropriate environments have been pre-selected for all notebooks and saved in their metadata if the needed environment doesn't yet exist, users will be prompted to change the kernel to one that does Note that an incorrect environment will likely be missing needed software and be incapable of running a notebook for which it was not intended Instead, create the needed environment using this notebook Code has been added to each notebook to check that it is running in the correct environment Warnings explain how to change to the correct environment if it has been created but the notebook isn't using it Warnings direct users to a notebook to create the environment if it does not yet exist There is also a minimal environment called \"scratch\" that is intended for user adaptation and experimentation This has been added for quick and easy access but users may add as many custom environments as they like","title":"What has changed?"},{"location":"release-notes/release_06-2021/#what-will-happen-if-i-dont-create-any-new-environments","text":"You will encounter environment warnings in the notebooks. You will not have access to the software needed to run the notebooks, which will trigger errors (ModuleNotFoundError).","title":"What will happen if I don't create any new environments?"},{"location":"release-notes/release_06-2021/#i-dont-want-to-wait-for-the-notebooks-to-yell-at-me-and-give-me-environment-warnings-how-can-i-create-the-environments-id-like-right-now","text":"Good choice! Head over to Create_OSL_Conda_Environments.ipynb and run the notebook. You will be prompted to select an environment from a list of options. Rerun the notebook for every environment you wish to add.","title":"I don't want to wait for the notebooks to yell at me and give me environment warnings. How can I create the environments I'd like right now?"},{"location":"release-notes/release_06-2021/#nbgitpuller-patch","text":"","title":"nbgitpuller Patch"},{"location":"release-notes/release_06-2021/#what-is-nbgitpuller","text":"nbgitpuller performs automatic merging of git repositories in a class-like setting. It handles merging in situations where instructor provided files may be edited both by students and/or the instructor. Its goal is to pull in an instructor's changes while preserving any edits a student has made. Where a conflict exists, it saves the student altered file with a timestamp appended to the filename and pulls in the instructor update.","title":"What is nbgitpuller?"},{"location":"release-notes/release_06-2021/#what-was-wrong-with-nbgitpuller","text":"nbgitpuller did not successfully handle all scenarios encountered in OpenSARlab. When it failed, users were locked out of their accounts. They had to login using a purpose-built profile that would skip the nbgitpuller. They could then identify and manually rectify the git state that caused the nbgitpuller to fail.","title":"What was wrong with nbgitpuller?"},{"location":"release-notes/release_06-2021/#how-was-this-issue-addressed","text":"We have populated user accounts with an altered version of nbgitpuller/pull.py - The nbgitpuller will no longer attempt to checkout files that have been removed from a remote branch - If the user has changed to a branch other than main in the opensarlab-notebooks git repository, the merge will be aborted - Users can still merge other branches from the command line","title":"How was this issue addressed?"},{"location":"release-notes/release_06-2021/#installed-jupyter-resource-usage","text":"","title":"Installed jupyter-resource-usage"},{"location":"release-notes/release_06-2021/#what-is-jupyter-resource-usage","text":"jupyter-resource-usage is an extension that displays how much memory a notebook server is using. The information is displayed at the top-right of every running Jupyter Notebook. It indicates the total memory used by all running notebooks, kernels, terminals, etc.","title":"What is jupyter-resource-usage?"},{"location":"release-notes/release_06-2021/#added-profile-identifier","text":"The name of the current profile now appears to the left of the \"Logout\" button on the home page.","title":"Added profile identifier"},{"location":"user-guides/OpenSARlab_environment/","text":"Return to Table of Contents The OpenSARlab Environment and Account Lifecycle Account Lifecycle Accounts will be deactivated on the 46th day of inactivity Warning emails are sent to inactive users after 30, 37, 41, 43, and 45 days The user volume and snapshot are permanently destroyed upon account deactivation OpenSARlab Environment Every OpenSARlab user has access to an Amazon AWS EC2 instance. Individual instances are shared among groups of 1 - 3 users, depending on demand. Operating System Ubuntu 18.04 Volume (storage) OpenSARlab uses Amazon AWS EBS volumes, mounted to user servers' home directories for storage. A snapshot of each volume is taken everyday at 10:00 UTC. Only the most recent snapshot is retained. Any volumes unused for two days are destroyed, with the latest snapshot retained as a backup. If a volume has been destroyed , a new EBS volume is created upon the next login, and it is populated with data from the snapshot. While the volume is created and becomes usable very quickly, populating it with data from the snapshot can take some time (tens of minutes) and users may notice that notebooks load slowly during this period. The important takeaway is that user storage is persistent; you will not lose saved work unless your account is deactivated after 46 days of non-use. It is incumbent upon users to manage their storage. Using up all your storage space will result in the inability to login to OpenSARlab. Please contact an OpenSARlab administrator if this occurs. - 500GB Amazon AWS EBS volume (volume size subject to change) Memory (RAM) EC2 instances are shared among users. This happens behind the scenes and is generally not noticeable when using OpenSARlab, with the exception of memory availability. The amount of memory available to each user fluctuates with overall use on an instance, and may vary from 6GB to 16GB. - 6GB - 16GB Privileges Users do not have root privileges (no sudo)","title":"OpenSARlab Account Details"},{"location":"user-guides/OpenSARlab_environment/#the-opensarlab-environment-and-account-lifecycle","text":"","title":"The OpenSARlab Environment and Account Lifecycle"},{"location":"user-guides/OpenSARlab_environment/#account-lifecycle","text":"Accounts will be deactivated on the 46th day of inactivity Warning emails are sent to inactive users after 30, 37, 41, 43, and 45 days The user volume and snapshot are permanently destroyed upon account deactivation","title":"Account Lifecycle"},{"location":"user-guides/OpenSARlab_environment/#opensarlab-environment","text":"Every OpenSARlab user has access to an Amazon AWS EC2 instance. Individual instances are shared among groups of 1 - 3 users, depending on demand.","title":"OpenSARlab Environment"},{"location":"user-guides/OpenSARlab_environment/#operating-system","text":"Ubuntu 18.04","title":"Operating System"},{"location":"user-guides/OpenSARlab_environment/#volume-storage","text":"OpenSARlab uses Amazon AWS EBS volumes, mounted to user servers' home directories for storage. A snapshot of each volume is taken everyday at 10:00 UTC. Only the most recent snapshot is retained. Any volumes unused for two days are destroyed, with the latest snapshot retained as a backup. If a volume has been destroyed , a new EBS volume is created upon the next login, and it is populated with data from the snapshot. While the volume is created and becomes usable very quickly, populating it with data from the snapshot can take some time (tens of minutes) and users may notice that notebooks load slowly during this period. The important takeaway is that user storage is persistent; you will not lose saved work unless your account is deactivated after 46 days of non-use. It is incumbent upon users to manage their storage. Using up all your storage space will result in the inability to login to OpenSARlab. Please contact an OpenSARlab administrator if this occurs. - 500GB Amazon AWS EBS volume (volume size subject to change)","title":"Volume (storage)"},{"location":"user-guides/OpenSARlab_environment/#memory-ram","text":"EC2 instances are shared among users. This happens behind the scenes and is generally not noticeable when using OpenSARlab, with the exception of memory availability. The amount of memory available to each user fluctuates with overall use on an instance, and may vary from 6GB to 16GB. - 6GB - 16GB","title":"Memory (RAM)"},{"location":"user-guides/OpenSARlab_environment/#privileges","text":"Users do not have root privileges (no sudo)","title":"Privileges"},{"location":"user-guides/OpenSARlab_terminal/","text":"Return to Table of Contents Using the Terminal in OpenSARlab Open a Terminal Select \"Terminal\" from the \"New\" menu in the JupyterHub file manager Use the Terminal Use the command line as you would in any other Linux terminal No Root Privileges OpenSARlab users do not have sudo privileges (there exists no jovyan password)","title":"OpenSARlab Terminal"},{"location":"user-guides/OpenSARlab_terminal/#using-the-terminal-in-opensarlab","text":"","title":"Using the Terminal in OpenSARlab"},{"location":"user-guides/OpenSARlab_terminal/#open-a-terminal","text":"Select \"Terminal\" from the \"New\" menu in the JupyterHub file manager","title":"Open a Terminal"},{"location":"user-guides/OpenSARlab_terminal/#use-the-terminal","text":"Use the command line as you would in any other Linux terminal","title":"Use the Terminal"},{"location":"user-guides/OpenSARlab_terminal/#no-root-privileges","text":"OpenSARlab users do not have sudo privileges (there exists no jovyan password)","title":"No Root Privileges"},{"location":"user-guides/class_notebooks_best_practices/","text":"Return to Table of Contents Developing Notebooks for Classes or Trainings: Best Practices Provide an Environments Capable of Running the Notebooks Provide students with a conda environment that contains all needed software. Conda Environments in OpenSARlab. If you distribute an environment.yml file, students can upload it to the /home/jovyan/conda_environments/Environment_Configs/ directory and then create it using the /home/jovyan/conda_environments/Create_OSL_Conda_Environments.ipynb notebook. If dependency conflicts prevent you from installing the software needed to run all your notebooks into a single environment, you will need to create multiple conda environments to run different notebooks. Test Notebooks Ahead of Time. If there are assignment sections requiring students to code or refactor code, test the notebook with the correct solutions in place. This will alert you to potential problems which you may not otherwise notice. Example: The notebook Plan for Students with Poor Internet Access File sizes for run notebooks containing a lot of output can be quite large. Users with slow internet connections may have difficulty saving notebooks in this state. It can be risky to require that students submit assignments in the form of saved, pre-run notebooks. Some students may simply not be able to save and turn in their work. Consider allowing assignments to be turned in as screenshots pasted into a Word or Google doc and saved as a pdf. Consider splitting assignments into 2 notebooks, one with content/examples and another for the assignment. Pass only needed data structures from the content notebook to the assignment notebook using a pickle.","title":"Class notebooks best practices"},{"location":"user-guides/class_notebooks_best_practices/#developing-notebooks-for-classes-or-trainings-best-practices","text":"","title":"Developing Notebooks for Classes or Trainings: Best Practices"},{"location":"user-guides/class_notebooks_best_practices/#provide-an-environments-capable-of-running-the-notebooks","text":"Provide students with a conda environment that contains all needed software. Conda Environments in OpenSARlab. If you distribute an environment.yml file, students can upload it to the /home/jovyan/conda_environments/Environment_Configs/ directory and then create it using the /home/jovyan/conda_environments/Create_OSL_Conda_Environments.ipynb notebook. If dependency conflicts prevent you from installing the software needed to run all your notebooks into a single environment, you will need to create multiple conda environments to run different notebooks.","title":"Provide an Environments Capable of Running the Notebooks"},{"location":"user-guides/class_notebooks_best_practices/#test-notebooks-ahead-of-time","text":"If there are assignment sections requiring students to code or refactor code, test the notebook with the correct solutions in place. This will alert you to potential problems which you may not otherwise notice. Example: The notebook","title":"Test Notebooks Ahead of Time."},{"location":"user-guides/class_notebooks_best_practices/#plan-for-students-with-poor-internet-access","text":"File sizes for run notebooks containing a lot of output can be quite large. Users with slow internet connections may have difficulty saving notebooks in this state. It can be risky to require that students submit assignments in the form of saved, pre-run notebooks. Some students may simply not be able to save and turn in their work. Consider allowing assignments to be turned in as screenshots pasted into a Word or Google doc and saved as a pdf. Consider splitting assignments into 2 notebooks, one with content/examples and another for the assignment. Pass only needed data structures from the content notebook to the assignment notebook using a pickle.","title":"Plan for Students with Poor Internet Access"},{"location":"user-guides/conda_environments/","text":"Return to Table of Contents Creating and Using Conda Environments in OpenSARlab OpenSARlab comes with a default base conda environment in which very little software is installed. Users must create conda environments in which to run Jupyter Notebooks or Python scripts. There are currently 5 conda environments provided by ASF to run the notebooks in our library: rtc_analysis insar_analysis train hydrosar machine learning These environments are not pre-built and must be created by each user. This is easily accomplished by running a notebook written for the purpose. The notebook can be found in your OpenSARlab account at the path: /home/jovyan/conda_environments/Create_OSL_Conda_Environments.ipynb Users can also create their own custom conda environments in OpenSARlab, and use them to run Jupyter Notebooks. Use the notebook found at this path in your OpenSARlab account to walk through the process: /home/jovyan/notebooks/OpenSARlab_supplements/Custom_Conda_Environments.ipynb","title":"Conda Environments"},{"location":"user-guides/conda_environments/#creating-and-using-conda-environments-in-opensarlab","text":"OpenSARlab comes with a default base conda environment in which very little software is installed. Users must create conda environments in which to run Jupyter Notebooks or Python scripts. There are currently 5 conda environments provided by ASF to run the notebooks in our library: rtc_analysis insar_analysis train hydrosar machine learning These environments are not pre-built and must be created by each user. This is easily accomplished by running a notebook written for the purpose. The notebook can be found in your OpenSARlab account at the path: /home/jovyan/conda_environments/Create_OSL_Conda_Environments.ipynb Users can also create their own custom conda environments in OpenSARlab, and use them to run Jupyter Notebooks. Use the notebook found at this path in your OpenSARlab account to walk through the process: /home/jovyan/notebooks/OpenSARlab_supplements/Custom_Conda_Environments.ipynb","title":"Creating and Using Conda Environments in OpenSARlab"},{"location":"user-guides/git_in_OpenSARlab/","text":"Return to Table of Contents Git in OpenSARlab ASF's Jupyter Notebook Library ASF's OpenSARlab Jupyter Notebook library lives in the asf-jupyter-notebooks GitHub repository . Gitpuller A smart git puller pulls in any changes to the repo each time a user's OpenSARlab server starts up. If a user has made changes to a notebook and the same notebook has been updated by ASF in the asf-jupyter-notebooks repo, the user will end up with two copies of that notebook. The user's version of the notebook will have a timestamp appended to its name. The notebook with the original, unaltered name will contain the new changes made by ASF. Please note that the gitpuller will not run if: You are not in the master branch of the asf-jupyter-notebook repo A file has been removed from the remote repo but still exists in your local repo What do I do if my git state is broken beyond my ability to repair it? Download or move any files you wish to preserve out of the /home/jovyan/notebooks directory (or its subdirectories) Delete the entire /home/jovyan/notebooks directory Open a terminal rm -rf /home/jovyan/notebooks Restart your server A fresh copy of the repo will be cloned into your account Using Other Git Repositories in OpenSARlab Users can use any repo they wish in OpenSARlab. It is best to clone any additional repos alongside or above the \"notebooks\" directory, which is where the asf-jupyter-notebooks repo is stored. This avoids issues that will arise from nesting repositories inside each other. Clone your repos to /home/jovyan , which is easily done in the terminal","title":"Git in OpenSARlab"},{"location":"user-guides/git_in_OpenSARlab/#git-in-opensarlab","text":"","title":"Git in OpenSARlab"},{"location":"user-guides/git_in_OpenSARlab/#asfs-jupyter-notebook-library","text":"ASF's OpenSARlab Jupyter Notebook library lives in the asf-jupyter-notebooks GitHub repository .","title":"ASF's Jupyter Notebook Library"},{"location":"user-guides/git_in_OpenSARlab/#gitpuller","text":"A smart git puller pulls in any changes to the repo each time a user's OpenSARlab server starts up. If a user has made changes to a notebook and the same notebook has been updated by ASF in the asf-jupyter-notebooks repo, the user will end up with two copies of that notebook. The user's version of the notebook will have a timestamp appended to its name. The notebook with the original, unaltered name will contain the new changes made by ASF. Please note that the gitpuller will not run if: You are not in the master branch of the asf-jupyter-notebook repo A file has been removed from the remote repo but still exists in your local repo","title":"Gitpuller"},{"location":"user-guides/git_in_OpenSARlab/#what-do-i-do-if-my-git-state-is-broken-beyond-my-ability-to-repair-it","text":"Download or move any files you wish to preserve out of the /home/jovyan/notebooks directory (or its subdirectories) Delete the entire /home/jovyan/notebooks directory Open a terminal rm -rf /home/jovyan/notebooks Restart your server A fresh copy of the repo will be cloned into your account","title":"What do I do if my git state is broken beyond my ability to repair it?"},{"location":"user-guides/git_in_OpenSARlab/#using-other-git-repositories-in-opensarlab","text":"Users can use any repo they wish in OpenSARlab. It is best to clone any additional repos alongside or above the \"notebooks\" directory, which is where the asf-jupyter-notebooks repo is stored. This avoids issues that will arise from nesting repositories inside each other. Clone your repos to /home/jovyan , which is easily done in the terminal","title":"Using Other Git Repositories in OpenSARlab"},{"location":"user-guides/how_to_run_a_notebook/","text":"Return to Table of Contents How to Run a Jupyter Notebook Before we get started, what is Jupyter Notebook? Intro to Jupyter Notebook Selecting Cells Select a Single Cell in Non-Edit Mode Click to the left of a cell A selected cell in non-edit mode is surrounded by a box with a blue left edge Select Multiple Cells in Non-Edit Mode Select a cell in non-edit mode 'Shift + J' or 'Shift + Down-Arrow' selects additional cells below 'Shift + K' or 'Shift + Up-Arrow' selects additional cells above Perform batch operations on selected cells Multiple selected cells are surrounded by a blue highlighted box Select a Code Cell in Edit Mode Click inside a cell A selected cell in non-edit mode is surrounded by a box with a green left edge Select a Markdown Cell in Edit Mode Double click inside a cell A markdown cell in edit mode is surrounded by a box with a green left edge Running Cells Since code cells may be run in any order, they are numbered in the order they were run. Run a Single Code or Markdown Cell With the Run Button Select a cell in edit or non-edit mode Click the Run button Click the Run button to run a selected cell With Hotkeys Select a cell in edit or non-edit mode 'Ctrl + Enter' runs a cell 'Shift + Enter' runs a cell and selects the cell below 'Alt + Enter' runs a cell and inserts an empty cell below Run a Cell and every Cell Above or Below It Select a cell in edit or non-edit mode Select 'Run All Above' from the Cell Menu Select 'Run All Below' from the Cell menu Run a batch of selected cells Select a group of cells Click the 'Run' Button Select 'Run Cells' from the Cells menu Run an Entire Notebook Select 'Run All' from the Cells Menu (does not restart the kernel) Select 'Restart & Run All' from the Kernel Menu Rerunning a Notebook It is a best-practice to restart the notebook kernel before rerunning a notebook. This is because any initialized variables and data structures from a previous run will be stored in memory, along with their values, which can lead to unintended results. An example of how you might get into trouble would be a code cell that creates a new subdirectory directory in your current working directory called \"data\" and moves into it. If you start in ~/notebooks, then after running the code cell once, you will be in ~/notebooks/data. If you run the cell a second time, you will be in ~notebooks/data/data. If you keep rerunning the cell, you will find yourself inside a deeply nested data directory, ~notebooks/data/data/data/data/data/data/. Select 'Restart' from the Kernel Menu Select 'Restart & Clear Output' from the Kernel Menu Select 'Restart & Run All' from the Kernel Menu Clearing Cell Output Before Closing It is best practice to clear all code cells output prior to closing or saving a notebook. Leaving the output in place can lead to a tenfold increase in a notebook's file size. Not only does this use up more of your volume, it leads to slow notebook loading times (especially if you have a slow internet connection).","title":"Running Jupyter Notebook"},{"location":"user-guides/how_to_run_a_notebook/#how-to-run-a-jupyter-notebook","text":"","title":"How to Run a Jupyter Notebook"},{"location":"user-guides/how_to_run_a_notebook/#before-we-get-started-what-is-jupyter-notebook","text":"Intro to Jupyter Notebook","title":"Before we get started, what is Jupyter Notebook?"},{"location":"user-guides/how_to_run_a_notebook/#selecting-cells","text":"","title":"Selecting Cells"},{"location":"user-guides/how_to_run_a_notebook/#select-a-single-cell-in-non-edit-mode","text":"Click to the left of a cell A selected cell in non-edit mode is surrounded by a box with a blue left edge","title":"Select a Single Cell in Non-Edit Mode"},{"location":"user-guides/how_to_run_a_notebook/#select-multiple-cells-in-non-edit-mode","text":"Select a cell in non-edit mode 'Shift + J' or 'Shift + Down-Arrow' selects additional cells below 'Shift + K' or 'Shift + Up-Arrow' selects additional cells above Perform batch operations on selected cells Multiple selected cells are surrounded by a blue highlighted box","title":"Select Multiple Cells in Non-Edit Mode"},{"location":"user-guides/how_to_run_a_notebook/#select-a-code-cell-in-edit-mode","text":"Click inside a cell A selected cell in non-edit mode is surrounded by a box with a green left edge","title":"Select a Code Cell in Edit Mode"},{"location":"user-guides/how_to_run_a_notebook/#select-a-markdown-cell-in-edit-mode","text":"Double click inside a cell A markdown cell in edit mode is surrounded by a box with a green left edge","title":"Select a Markdown Cell in Edit Mode"},{"location":"user-guides/how_to_run_a_notebook/#running-cells","text":"Since code cells may be run in any order, they are numbered in the order they were run.","title":"Running Cells"},{"location":"user-guides/how_to_run_a_notebook/#run-a-single-code-or-markdown-cell","text":"","title":"Run a Single Code or Markdown Cell"},{"location":"user-guides/how_to_run_a_notebook/#with-the-run-button","text":"Select a cell in edit or non-edit mode Click the Run button Click the Run button to run a selected cell","title":"With the Run Button"},{"location":"user-guides/how_to_run_a_notebook/#with-hotkeys","text":"Select a cell in edit or non-edit mode 'Ctrl + Enter' runs a cell 'Shift + Enter' runs a cell and selects the cell below 'Alt + Enter' runs a cell and inserts an empty cell below","title":"With Hotkeys"},{"location":"user-guides/how_to_run_a_notebook/#run-a-cell-and-every-cell-above-or-below-it","text":"Select a cell in edit or non-edit mode Select 'Run All Above' from the Cell Menu Select 'Run All Below' from the Cell menu","title":"Run a Cell and every Cell Above or Below It"},{"location":"user-guides/how_to_run_a_notebook/#run-a-batch-of-selected-cells","text":"Select a group of cells Click the 'Run' Button Select 'Run Cells' from the Cells menu","title":"Run a batch of selected cells"},{"location":"user-guides/how_to_run_a_notebook/#run-an-entire-notebook","text":"Select 'Run All' from the Cells Menu (does not restart the kernel) Select 'Restart & Run All' from the Kernel Menu","title":"Run an Entire Notebook"},{"location":"user-guides/how_to_run_a_notebook/#rerunning-a-notebook","text":"It is a best-practice to restart the notebook kernel before rerunning a notebook. This is because any initialized variables and data structures from a previous run will be stored in memory, along with their values, which can lead to unintended results. An example of how you might get into trouble would be a code cell that creates a new subdirectory directory in your current working directory called \"data\" and moves into it. If you start in ~/notebooks, then after running the code cell once, you will be in ~/notebooks/data. If you run the cell a second time, you will be in ~notebooks/data/data. If you keep rerunning the cell, you will find yourself inside a deeply nested data directory, ~notebooks/data/data/data/data/data/data/. Select 'Restart' from the Kernel Menu Select 'Restart & Clear Output' from the Kernel Menu Select 'Restart & Run All' from the Kernel Menu","title":"Rerunning a Notebook"},{"location":"user-guides/how_to_run_a_notebook/#clearing-cell-output-before-closing","text":"It is best practice to clear all code cells output prior to closing or saving a notebook. Leaving the output in place can lead to a tenfold increase in a notebook's file size. Not only does this use up more of your volume, it leads to slow notebook loading times (especially if you have a slow internet connection).","title":"Clearing Cell Output Before Closing"},{"location":"user-guides/installing_software_in_OpenSARlab/","text":"Return to Table of Contents Installing Software in OpenSARlab pip You can install pip packages to your /home/jovyan/.local/lib/python3.7/site-packages directory Open a terminal python -m pip install --user <package_name> To install a pip package inside of a conda environment Open a terminal conda activate <environment_name> python -m pip install --user <package_name> apt and apt-get At this time, users cannot install software in OpenSARlab using apt or apt-get. conda Users can install additional software with conda in OpenSARlab, but it will not persist after the sever shuts down. It will need to be reinstalled during subsequent OpenSARlab sessions. Install conda packages from within a notebook running in an environment Edit a notebook code cell %conda install <package_name> Run the code cell Install conda packages from the terminal Open a terminal conda activate <environment_name> conda install <package_name> conda install package_name","title":"Installing Software in OpenSARlab"},{"location":"user-guides/installing_software_in_OpenSARlab/#installing-software-in-opensarlab","text":"","title":"Installing Software in OpenSARlab"},{"location":"user-guides/installing_software_in_OpenSARlab/#pip","text":"","title":"pip"},{"location":"user-guides/installing_software_in_OpenSARlab/#you-can-install-pip-packages-to-your-homejovyanlocallibpython37site-packages-directory","text":"Open a terminal python -m pip install --user <package_name>","title":"You can install pip packages to your /home/jovyan/.local/lib/python3.7/site-packages directory"},{"location":"user-guides/installing_software_in_OpenSARlab/#to-install-a-pip-package-inside-of-a-conda-environment","text":"Open a terminal conda activate <environment_name> python -m pip install --user <package_name>","title":"To install a pip package inside of a conda environment"},{"location":"user-guides/installing_software_in_OpenSARlab/#apt-and-apt-get","text":"At this time, users cannot install software in OpenSARlab using apt or apt-get.","title":"apt and apt-get"},{"location":"user-guides/installing_software_in_OpenSARlab/#conda","text":"Users can install additional software with conda in OpenSARlab, but it will not persist after the sever shuts down. It will need to be reinstalled during subsequent OpenSARlab sessions.","title":"conda"},{"location":"user-guides/installing_software_in_OpenSARlab/#install-conda-packages-from-within-a-notebook-running-in-an-environment","text":"Edit a notebook code cell %conda install <package_name> Run the code cell","title":"Install conda packages from within a notebook running in an environment"},{"location":"user-guides/installing_software_in_OpenSARlab/#install-conda-packages-from-the-terminal","text":"Open a terminal conda activate <environment_name> conda install <package_name> conda install package_name","title":"Install conda packages from the terminal"},{"location":"user-guides/jupyter_magic/","text":"Return to Table of Contents Jupyter Line and Cell Magics, and IPython Syntax Jupyter Notebook magic commands provide shortcuts and functionality to the notebooks, above what can be done with straight Python code. An exhaustive list of magics can be found in the IPython docs . While all magics are available to users, in OpenSARlab we tend to use a relatively small selection of them. These fall into two categories: line magics and cell magics. We also frequently use Ipython's shell assignment syntax. IPython Shell Assignment with \"!\" In IPython syntax the exclamation mark allows users to run shell commands from inside a Jupyter Notebook code cell. Simply start a line of code with \"!\" and it will run the command in the shell. Example: !pwd will print the current working directory. Line Magics Line magics start with a single \"%\" and effect only the line on which they are used. %matplotlib inline Allows non-interactive matplotlib plots to be displayed in a notebook. %matplotlib notebook Allows interactive matplotlib plots to be displayed and interacted with inside of a notebook. %df This is a custom magic written for OpenSARlab. It uses the python function shutil.disk_usage() to check the state of storage on user volumes. \"%df\" returns a human readable string in GB. \"%df --raw\" returns a raw data object \"%df --on\" returns a string in GB after every subsequent code cell is run \"%df --off\" turns \"%df --on\" back off \"%df -v\" prints additional debugging text Cell Magics Cell magics start with \"%%\" and effect the contents of an entire cell. %%javascript or %%js Runs a code cell of javascript code. Note: leave a blank line above the magic command at the top of the code cell. %%capture Runs the cell but captures all output. We typically use this to suppress the display of a matplotlib plot we do not wish to show.","title":"Jupyter Magic Commands"},{"location":"user-guides/jupyter_magic/#jupyter-line-and-cell-magics-and-ipython-syntax","text":"Jupyter Notebook magic commands provide shortcuts and functionality to the notebooks, above what can be done with straight Python code. An exhaustive list of magics can be found in the IPython docs . While all magics are available to users, in OpenSARlab we tend to use a relatively small selection of them. These fall into two categories: line magics and cell magics. We also frequently use Ipython's shell assignment syntax.","title":"Jupyter Line and Cell Magics, and IPython Syntax"},{"location":"user-guides/jupyter_magic/#ipython-shell-assignment-with","text":"In IPython syntax the exclamation mark allows users to run shell commands from inside a Jupyter Notebook code cell. Simply start a line of code with \"!\" and it will run the command in the shell. Example: !pwd will print the current working directory.","title":"IPython Shell Assignment with \"!\""},{"location":"user-guides/jupyter_magic/#line-magics","text":"Line magics start with a single \"%\" and effect only the line on which they are used.","title":"Line Magics"},{"location":"user-guides/jupyter_magic/#matplotlib-inline","text":"Allows non-interactive matplotlib plots to be displayed in a notebook.","title":"%matplotlib inline"},{"location":"user-guides/jupyter_magic/#matplotlib-notebook","text":"Allows interactive matplotlib plots to be displayed and interacted with inside of a notebook.","title":"%matplotlib notebook"},{"location":"user-guides/jupyter_magic/#df","text":"This is a custom magic written for OpenSARlab. It uses the python function shutil.disk_usage() to check the state of storage on user volumes. \"%df\" returns a human readable string in GB. \"%df --raw\" returns a raw data object \"%df --on\" returns a string in GB after every subsequent code cell is run \"%df --off\" turns \"%df --on\" back off \"%df -v\" prints additional debugging text","title":"%df"},{"location":"user-guides/jupyter_magic/#cell-magics","text":"Cell magics start with \"%%\" and effect the contents of an entire cell.","title":"Cell Magics"},{"location":"user-guides/jupyter_magic/#javascript-or-js","text":"Runs a code cell of javascript code. Note: leave a blank line above the magic command at the top of the code cell.","title":"%%javascript or %%js"},{"location":"user-guides/jupyter_magic/#capture","text":"Runs the cell but captures all output. We typically use this to suppress the display of a matplotlib plot we do not wish to show.","title":"%%capture"},{"location":"user-guides/jupyter_notebook_extensions/","text":"Return to Table of Contents Managing Jupyter Notebook Extensions As an OpenSARlab user, you have access to all of the notebook extensions available in the nbextensions package. A detailed list of included extensions is available here . Enabling and Disabling Extensions The easiest way to manage notebook extensions is via the nbextensions tab. Click the nbextensions tab from the file manager Once the nbextensions tab is open, you can select individual extensions to learn how they function and enable or disable them. Select an extension to learn more about and click the \"Enable\" or \"Disable\" buttons to manage its use","title":"Jupyter Notebook Extensions"},{"location":"user-guides/jupyter_notebook_extensions/#managing-jupyter-notebook-extensions","text":"As an OpenSARlab user, you have access to all of the notebook extensions available in the nbextensions package. A detailed list of included extensions is available here .","title":"Managing Jupyter Notebook Extensions"},{"location":"user-guides/jupyter_notebook_extensions/#enabling-and-disabling-extensions","text":"The easiest way to manage notebook extensions is via the nbextensions tab. Click the nbextensions tab from the file manager Once the nbextensions tab is open, you can select individual extensions to learn how they function and enable or disable them. Select an extension to learn more about and click the \"Enable\" or \"Disable\" buttons to manage its use","title":"Enabling and Disabling Extensions"},{"location":"user-guides/jupyter_notebook_intro/","text":"Return to Table of Contents A Light Introduction to Jupyter Notebook https://jupyter.org/ Jupyter Notebook is a web application that allows users to display interactive, runnable code cells (typically written in Python) alongside markdown cells containing explanatory text, formulas, hyperlinks, tables, pseudocode, and images. Jupyter Notebook provides an ideal format for teaching/learning coding concepts, prototyping algorithms, and collaborating on Python projects. Cells Jupyter Notebook has 4 cell types. In OpenSARlab, we use two of them, \"markdown cells\" and \"code cells.\" Markdown Cells Markdown cells contain documentation in Markdown, HTML, and/or Latex. They may contain text, images, hyperlinks, formulas, tables, pseudocode, plots, and figures. To enter edit mode in a markdown cell, double click it. A markdown cell in edit mode To move past or display a markdown cell's content, run it. A run markdown cell Code Cells Code cells contain editable, runnable Python code. You can run them in any order, any number of times. The ability to run and rerun code cells in arbitrary order can be helpful but can also cause problems. For instance, recycled variables can end up with unexpected values if cells are run in an order not intended by a notebook's author. A code cell","title":"Jupyter Notebook Intro"},{"location":"user-guides/jupyter_notebook_intro/#a-light-introduction-to-jupyter-notebook","text":"https://jupyter.org/ Jupyter Notebook is a web application that allows users to display interactive, runnable code cells (typically written in Python) alongside markdown cells containing explanatory text, formulas, hyperlinks, tables, pseudocode, and images. Jupyter Notebook provides an ideal format for teaching/learning coding concepts, prototyping algorithms, and collaborating on Python projects.","title":"A Light Introduction to Jupyter Notebook"},{"location":"user-guides/jupyter_notebook_intro/#cells","text":"Jupyter Notebook has 4 cell types. In OpenSARlab, we use two of them, \"markdown cells\" and \"code cells.\"","title":"Cells"},{"location":"user-guides/jupyter_notebook_intro/#markdown-cells","text":"Markdown cells contain documentation in Markdown, HTML, and/or Latex. They may contain text, images, hyperlinks, formulas, tables, pseudocode, plots, and figures. To enter edit mode in a markdown cell, double click it. A markdown cell in edit mode To move past or display a markdown cell's content, run it. A run markdown cell","title":"Markdown Cells"},{"location":"user-guides/jupyter_notebook_intro/#code-cells","text":"Code cells contain editable, runnable Python code. You can run them in any order, any number of times. The ability to run and rerun code cells in arbitrary order can be helpful but can also cause problems. For instance, recycled variables can end up with unexpected values if cells are run in an order not intended by a notebook's author. A code cell","title":"Code Cells"},{"location":"user-guides/logging_out_and_server_shutdown/","text":"Return to Table of Contents Logging Out of OpenSARlab and Shutting Down the Server When you are ready to stop working in OpenSARlab, it is best practice to shut down your server and also logout. Why Shut Down the Server? Logging out will not shut down the server. If the server is inactive for an hour, it will shut down automatically, but users should not rely upon this feature. If there are any running processes in your account, they will keep the server alive, and it is not always apparent that there is a process running. If you login for the first time in a day or two and notice that a Jupyter Notebook is still running, your server ran that whole time. Do your part to reduce resource use and ease the burden on the environment by shutting down your server when you are finished working for the day. In some instances you may need to leave your server running, which is perfectly acceptable. Perhaps you have a notebook performing a very time intensive analysis and wish to let it run overnight. How to Shut Down The Server and Logout Click The Control Panel Button Click the Control Panel button at the top right of the file manager or in an open notebook Click The Stop My Server Button Click the Stop My Server button that appears Click The Logout Button Click the Logout button","title":"Logging Out and Server Shutdown"},{"location":"user-guides/logging_out_and_server_shutdown/#logging-out-of-opensarlab-and-shutting-down-the-server","text":"When you are ready to stop working in OpenSARlab, it is best practice to shut down your server and also logout.","title":"Logging Out of OpenSARlab and Shutting Down the Server"},{"location":"user-guides/logging_out_and_server_shutdown/#why-shut-down-the-server","text":"Logging out will not shut down the server. If the server is inactive for an hour, it will shut down automatically, but users should not rely upon this feature. If there are any running processes in your account, they will keep the server alive, and it is not always apparent that there is a process running. If you login for the first time in a day or two and notice that a Jupyter Notebook is still running, your server ran that whole time. Do your part to reduce resource use and ease the burden on the environment by shutting down your server when you are finished working for the day. In some instances you may need to leave your server running, which is perfectly acceptable. Perhaps you have a notebook performing a very time intensive analysis and wish to let it run overnight.","title":"Why Shut Down the Server?"},{"location":"user-guides/logging_out_and_server_shutdown/#how-to-shut-down-the-server-and-logout","text":"","title":"How to Shut Down The Server and Logout"},{"location":"user-guides/logging_out_and_server_shutdown/#click-the-control-panel-button","text":"Click the Control Panel button at the top right of the file manager or in an open notebook","title":"Click The Control Panel Button"},{"location":"user-guides/logging_out_and_server_shutdown/#click-the-stop-my-server-button","text":"Click the Stop My Server button that appears","title":"Click The Stop My Server Button"},{"location":"user-guides/logging_out_and_server_shutdown/#click-the-logout-button","text":"Click the Logout button","title":"Click The Logout Button"},{"location":"user-guides/restarting_server_and_kernel/","text":"Return to Table of Contents Restarting the OpenSARlab Server and Notebook Kernel Restarting the OpenSARlab Server Why? Restarting the server triggers the git puller to run. Perhaps you have deleted or altered a notebook in the ASF notebook library and want to retrieve the original. Maybe you know that a notebook update was just made and you would like to immediately pull in changes from the asf-jupyter-notebook repo . A quick solution in either of those cases is to restart your server. Note: If you are comfortable with git, you could instead do a git pull from the terminal or in a notebook. How? Click The Control Panel Button Click the Control Panel button at the top right of the file manager or in an open notebook Click The Stop My Server Button Click the Stop My Server button that appears Click The Start My Server Button Click the Start My Server button, which may take a few seconds to appear Click The Launch Server Button Click the Launch Server button appears Select a Server Option and Click Start Select a server option and click the start button Wait For the Server To Start Wait for the server to start Optional: Click The Event Log Arrow For Detailed Startup Status Information Click the event log arrow to view logs documenting the status of the startup process Restarting a Jupyter Notebook Kernel Why? As you run code cells in a notebook, initialized variables and their assigned values are stored in memory. If you decide to start over and re-run a previously run notebook, without first restarting the kernel, all of the previously defined variables and values will still persist in memory. Not only are they using up some of an instance's limited memory allotment, but previously defined values may cause unintended results when re-running the code. The solution is to restart the kernel, clearing notebook data stored in memory. Example: Imagine a notebook that builds a string, which starts empty and then has substrings systematically appended to it. When re-running this notebook, the string will no longer start empty and the resultant string will contain an unintended substring at its start. How? Select \"Restart\", \"Restart & Clear Output\", or \"Restart & Run All\" From The Kernel Menu For most use cases, select \"Restart & Clear Output\" \"Restart\" will restart the kernel but leave old code cell output in place. \"Restart & Clear Output\" restarts the kernel and removes old code cell output. This is generally the preferred option. \"Restart & Run All\" restarts the kernel and runs all the code cells. This only works if the notebook takes no user input.","title":"OpenSARlab Servers and Kernels"},{"location":"user-guides/restarting_server_and_kernel/#restarting-the-opensarlab-server-and-notebook-kernel","text":"","title":"Restarting the OpenSARlab Server and Notebook Kernel"},{"location":"user-guides/restarting_server_and_kernel/#restarting-the-opensarlab-server","text":"","title":"Restarting the OpenSARlab Server"},{"location":"user-guides/restarting_server_and_kernel/#why","text":"Restarting the server triggers the git puller to run. Perhaps you have deleted or altered a notebook in the ASF notebook library and want to retrieve the original. Maybe you know that a notebook update was just made and you would like to immediately pull in changes from the asf-jupyter-notebook repo . A quick solution in either of those cases is to restart your server. Note: If you are comfortable with git, you could instead do a git pull from the terminal or in a notebook.","title":"Why?"},{"location":"user-guides/restarting_server_and_kernel/#how","text":"","title":"How?"},{"location":"user-guides/restarting_server_and_kernel/#click-the-control-panel-button","text":"Click the Control Panel button at the top right of the file manager or in an open notebook","title":"Click The Control Panel Button"},{"location":"user-guides/restarting_server_and_kernel/#click-the-stop-my-server-button","text":"Click the Stop My Server button that appears","title":"Click The Stop My Server Button"},{"location":"user-guides/restarting_server_and_kernel/#click-the-start-my-server-button","text":"Click the Start My Server button, which may take a few seconds to appear","title":"Click The Start My Server Button"},{"location":"user-guides/restarting_server_and_kernel/#click-the-launch-server-button","text":"Click the Launch Server button appears","title":"Click The Launch Server Button"},{"location":"user-guides/restarting_server_and_kernel/#select-a-server-option-and-click-start","text":"Select a server option and click the start button","title":"Select a Server Option and Click Start"},{"location":"user-guides/restarting_server_and_kernel/#wait-for-the-server-to-start","text":"Wait for the server to start","title":"Wait For the Server To Start"},{"location":"user-guides/restarting_server_and_kernel/#optional-click-the-event-log-arrow-for-detailed-startup-status-information","text":"Click the event log arrow to view logs documenting the status of the startup process","title":"Optional: Click The Event Log Arrow For Detailed Startup Status Information"},{"location":"user-guides/restarting_server_and_kernel/#restarting-a-jupyter-notebook-kernel","text":"","title":"Restarting a Jupyter Notebook Kernel"},{"location":"user-guides/restarting_server_and_kernel/#why_1","text":"As you run code cells in a notebook, initialized variables and their assigned values are stored in memory. If you decide to start over and re-run a previously run notebook, without first restarting the kernel, all of the previously defined variables and values will still persist in memory. Not only are they using up some of an instance's limited memory allotment, but previously defined values may cause unintended results when re-running the code. The solution is to restart the kernel, clearing notebook data stored in memory. Example: Imagine a notebook that builds a string, which starts empty and then has substrings systematically appended to it. When re-running this notebook, the string will no longer start empty and the resultant string will contain an unintended substring at its start.","title":"Why?"},{"location":"user-guides/restarting_server_and_kernel/#how_1","text":"","title":"How?"},{"location":"user-guides/restarting_server_and_kernel/#select-restart-restart-clear-output-or-restart-run-all-from-the-kernel-menu","text":"For most use cases, select \"Restart & Clear Output\" \"Restart\" will restart the kernel but leave old code cell output in place. \"Restart & Clear Output\" restarts the kernel and removes old code cell output. This is generally the preferred option. \"Restart & Run All\" restarts the kernel and runs all the code cells. This only works if the notebook takes no user input.","title":"Select \"Restart\", \"Restart &amp; Clear Output\", or \"Restart &amp; Run All\" From The Kernel Menu"},{"location":"user-guides/troubleshooting_guide/","text":"Return to Table of Contents Troubleshooting Guide Why did the kernel die while running a notebook? The message that appears when a notebook kernel dies The kernel will die if you run out of available memory to complete a running process. This most frequently occurs when running a time-series or change detection algorithm on a data stack that is either too deep or covers too large an area-of-interest for OpenSARlab to handle. Try running the notebook on some combination of a shallower data stack and/or a smaller area-of-interest. This may take some experimentation because memory is shared among users and the amount available fluctuates. To work with a deep stack covering an extensive AOI, you may need to tile up your data for the analysis and mosaic them later. I successfully ran a notebook earlier on the same data but now it is killing the kernel. OpenSARlab EC2 instances are shared among 1-3 users. The memory available to each user on an instance varies with overall activity on the EC2. It is likely that there was enough memory available for your process the first time you attempted it, but there wasn't on the subsequent attempt. More details on the OpenSARlab user environment can be found here . When I open a notebook, I receive \"Kernel not found\" message. The message that appears when a notebook kernel cannot be found You have either not yet created the required conda environment or there is a mix-up between the environment name and prefix. If you think you did already install the environment, select it from the pull-down menu and click the \"Set Kernel\" button. If you have not yet created it, use the notebook located here to do so: /home/jovyan/conda_environments/Create_OSL_Conda_Environments.ipynb I am receiving a \"No space left on device\" error. OpenSARlab users have access to a finite amount of storage space ( details here ). It is up to users to manage their storage. If you receive a storage space warning while logged into OpenSARlab, it is highly recommended that you immediately free up space by deleting un-needed files. If your server shuts down while there is no available space, it will not have the space needed to restart again and you will be locked out of your account. If you you become locked out of your account, contact an OpenSARlab administrator for help. They will assign just enough extra storage to allow you to login and delete files. If you do not have any extraneous files to delete and feel that you really do need additional space to do your work, contact an OpenSARlab administrator and request an increased storage limit. Limits will only be increased if there is a demonstrable need. My server won't start and I cannot access OpenSARlab. This issue typically stems from unexpected behavior on the part of the smart git puller . Click the event log arrow beneath the server startup progress bar to view the details of any git puller conflicts. Click the event log arrow beneath the server startup progress bar. If the problem is related to the git puller, you will find details regarding which file or files are causing the conflict in the event log. If the problem is not related to the git puller, contact an OpenSARlab Administrator . If the problem is git puller related, note the names and locations of the offending files and logout of OpenSARlab. Click the logout button at the top of the screen After logging out, the startup screen will reload. Select the \"General SAR processing (without git puller)\" server option and click the start button. Select the \"General SAR processing (without git puller)\" server option and click the start button The server should now load and you will have access to your account. Navigate to the locations of the conflicting files. There are three options for dealing with each of the offending files: Delete the file if there are no changes from the original that you wish to save. Rename the file if there are changes you wish to save. From the terminal , run \"touch /path/to/your.file\" if you wish to try updating the file's timestamp and give the git puller another opportunity to handle the git pull correctly. Logout of OpenSARlab. Click the logout button at the top right of the screen Log back in, using the \"General SAR processing\" server option. Select the \"General SAR processing\" option and click start The git puller should now run successfully, the server should startup, and you should have restored access to OpenSARlab, receiving any updates to the ASF notebook library . The edits I made to an ASF notebook have disappeared since the last time I used OpenSARlab. When your OpenSARlab server starts up, a git puller runs, bringing in any updates made to the ASF notebook library . If a change has been made to a notebook by both the user and by ASF, both will be saved. The ASF version will retain its original name. The user edited version will have a timestamp appended to its name. The edited notebook you are missing is likely in the location where you expect it but has a recent timestamp appended to its name. One of my notebooks looks like it has a mix of code from various versions of the notebook. We have seen this happen occasionally and it stems from issues with the smart git puller . The best option is to delete the notebook and restart your OpenSARlab server . The notebook will be replaced with a fresh copy from the ASF notebook library . I know there was an update made to an ASF notebook but I still have the old version. We have seen this happen occasionally and it stems from issues with the smart git puller . The best option is to delete the outdated version of the notebook and restart your OpenSARlab server . The notebook will be replaced with a fresh copy from the ASF notebook library . I am having trouble setting up a web server and developing my web app in OpenSARlab. This cannot be done in OpenSARlab. You will need to develop your app in an environment where you can run your web server. A notebook won't load. A new browser tab opens and shows the JupyterHub header, but no notebook appears. This is caused by the slow loading of a large notebook. If you run a notebook and close it without first clearing all code cell output, the file size can increase substantially. A 40KB notebook can grow to 60MB+ file if its output is left in place. The notebook will eventually load. You may need to reload your browser window if it times out. My issue is not on this list If you have encountered an issue not covered in this document, please contact an OpenSARlab administrator for help.","title":"Troubleshooting Guide"},{"location":"user-guides/troubleshooting_guide/#troubleshooting-guide","text":"","title":"Troubleshooting Guide"},{"location":"user-guides/troubleshooting_guide/#why-did-the-kernel-die-while-running-a-notebook","text":"The message that appears when a notebook kernel dies The kernel will die if you run out of available memory to complete a running process. This most frequently occurs when running a time-series or change detection algorithm on a data stack that is either too deep or covers too large an area-of-interest for OpenSARlab to handle. Try running the notebook on some combination of a shallower data stack and/or a smaller area-of-interest. This may take some experimentation because memory is shared among users and the amount available fluctuates. To work with a deep stack covering an extensive AOI, you may need to tile up your data for the analysis and mosaic them later.","title":"Why did the kernel die while running a notebook?"},{"location":"user-guides/troubleshooting_guide/#i-successfully-ran-a-notebook-earlier-on-the-same-data-but-now-it-is-killing-the-kernel","text":"OpenSARlab EC2 instances are shared among 1-3 users. The memory available to each user on an instance varies with overall activity on the EC2. It is likely that there was enough memory available for your process the first time you attempted it, but there wasn't on the subsequent attempt. More details on the OpenSARlab user environment can be found here .","title":"I successfully ran a notebook earlier on the same data but now it is killing the kernel."},{"location":"user-guides/troubleshooting_guide/#when-i-open-a-notebook-i-receive-kernel-not-found-message","text":"The message that appears when a notebook kernel cannot be found You have either not yet created the required conda environment or there is a mix-up between the environment name and prefix. If you think you did already install the environment, select it from the pull-down menu and click the \"Set Kernel\" button. If you have not yet created it, use the notebook located here to do so: /home/jovyan/conda_environments/Create_OSL_Conda_Environments.ipynb","title":"When I open a notebook, I receive \"Kernel not found\" message."},{"location":"user-guides/troubleshooting_guide/#i-am-receiving-a-no-space-left-on-device-error","text":"OpenSARlab users have access to a finite amount of storage space ( details here ). It is up to users to manage their storage. If you receive a storage space warning while logged into OpenSARlab, it is highly recommended that you immediately free up space by deleting un-needed files. If your server shuts down while there is no available space, it will not have the space needed to restart again and you will be locked out of your account. If you you become locked out of your account, contact an OpenSARlab administrator for help. They will assign just enough extra storage to allow you to login and delete files. If you do not have any extraneous files to delete and feel that you really do need additional space to do your work, contact an OpenSARlab administrator and request an increased storage limit. Limits will only be increased if there is a demonstrable need.","title":"I am receiving a \"No space left on device\" error."},{"location":"user-guides/troubleshooting_guide/#my-server-wont-start-and-i-cannot-access-opensarlab","text":"This issue typically stems from unexpected behavior on the part of the smart git puller . Click the event log arrow beneath the server startup progress bar to view the details of any git puller conflicts. Click the event log arrow beneath the server startup progress bar. If the problem is related to the git puller, you will find details regarding which file or files are causing the conflict in the event log. If the problem is not related to the git puller, contact an OpenSARlab Administrator . If the problem is git puller related, note the names and locations of the offending files and logout of OpenSARlab. Click the logout button at the top of the screen After logging out, the startup screen will reload. Select the \"General SAR processing (without git puller)\" server option and click the start button. Select the \"General SAR processing (without git puller)\" server option and click the start button The server should now load and you will have access to your account. Navigate to the locations of the conflicting files. There are three options for dealing with each of the offending files: Delete the file if there are no changes from the original that you wish to save. Rename the file if there are changes you wish to save. From the terminal , run \"touch /path/to/your.file\" if you wish to try updating the file's timestamp and give the git puller another opportunity to handle the git pull correctly. Logout of OpenSARlab. Click the logout button at the top right of the screen Log back in, using the \"General SAR processing\" server option. Select the \"General SAR processing\" option and click start The git puller should now run successfully, the server should startup, and you should have restored access to OpenSARlab, receiving any updates to the ASF notebook library .","title":"My server won't start and I cannot access OpenSARlab."},{"location":"user-guides/troubleshooting_guide/#the-edits-i-made-to-an-asf-notebook-have-disappeared-since-the-last-time-i-used-opensarlab","text":"When your OpenSARlab server starts up, a git puller runs, bringing in any updates made to the ASF notebook library . If a change has been made to a notebook by both the user and by ASF, both will be saved. The ASF version will retain its original name. The user edited version will have a timestamp appended to its name. The edited notebook you are missing is likely in the location where you expect it but has a recent timestamp appended to its name.","title":"The edits I made to an ASF notebook have disappeared since the last time I used OpenSARlab."},{"location":"user-guides/troubleshooting_guide/#one-of-my-notebooks-looks-like-it-has-a-mix-of-code-from-various-versions-of-the-notebook","text":"We have seen this happen occasionally and it stems from issues with the smart git puller . The best option is to delete the notebook and restart your OpenSARlab server . The notebook will be replaced with a fresh copy from the ASF notebook library .","title":"One of my notebooks looks like it has a mix of code from various versions of the notebook."},{"location":"user-guides/troubleshooting_guide/#i-know-there-was-an-update-made-to-an-asf-notebook-but-i-still-have-the-old-version","text":"We have seen this happen occasionally and it stems from issues with the smart git puller . The best option is to delete the outdated version of the notebook and restart your OpenSARlab server . The notebook will be replaced with a fresh copy from the ASF notebook library .","title":"I know there was an update made to an ASF notebook but I still have the old version."},{"location":"user-guides/troubleshooting_guide/#i-am-having-trouble-setting-up-a-web-server-and-developing-my-web-app-in-opensarlab","text":"This cannot be done in OpenSARlab. You will need to develop your app in an environment where you can run your web server.","title":"I am having trouble setting up a web server and developing my web app in OpenSARlab."},{"location":"user-guides/troubleshooting_guide/#a-notebook-wont-load-a-new-browser-tab-opens-and-shows-the-jupyterhub-header-but-no-notebook-appears","text":"This is caused by the slow loading of a large notebook. If you run a notebook and close it without first clearing all code cell output, the file size can increase substantially. A 40KB notebook can grow to 60MB+ file if its output is left in place. The notebook will eventually load. You may need to reload your browser window if it times out.","title":"A notebook won't load. A new browser tab opens and shows the JupyterHub header, but no notebook appears."},{"location":"user-guides/troubleshooting_guide/#my-issue-is-not-on-this-list","text":"If you have encountered an issue not covered in this document, please contact an OpenSARlab administrator for help.","title":"My issue is not on this list"}]}